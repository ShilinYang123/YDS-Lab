#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YDS-Lab æ—¥å¿—ç®¡ç†å·¥å…·
æä¾›ç»Ÿä¸€çš„æ—¥å¿—ç®¡ç†ã€åˆ†æã€æ¸…ç†å’Œç›‘æ§åŠŸèƒ½
é€‚é…YDS-Labé¡¹ç›®ç»“æ„å’ŒAI Agentåä½œéœ€æ±‚
"""

import os
import sys
import json
import yaml
import logging
from logging import handlers as logging_handlers
import re
import gzip
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import threading
import time
from collections import defaultdict, Counter
import subprocess

try:
    import colorlog
    COLORLOG_AVAILABLE = True
except ImportError:
    COLORLOG_AVAILABLE = False

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    timestamp: datetime
    level: str
    logger: str
    message: str
    file_path: str = ''
    line_number: int = 0
    function: str = ''
    thread_id: str = ''
    process_id: int = 0
    extra_data: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.extra_data is None:
            self.extra_data = {}

@dataclass
class LogStats:
    """æ—¥å¿—ç»Ÿè®¡"""
    total_entries: int = 0
    level_counts: Dict[str, int] = None
    logger_counts: Dict[str, int] = None
    error_patterns: Dict[str, int] = None
    time_range: Tuple[datetime, datetime] = None
    file_sizes: Dict[str, int] = None
    
    def __post_init__(self):
        if self.level_counts is None:
            self.level_counts = {}
        if self.logger_counts is None:
            self.logger_counts = {}
        if self.error_patterns is None:
            self.error_patterns = {}
        if self.file_sizes is None:
            self.file_sizes = {}

@dataclass
class LogAlert:
    """æ—¥å¿—å‘Šè­¦"""
    id: str
    level: str
    pattern: str
    threshold: int
    time_window: int  # åˆ†é’Ÿ
    count: int = 0
    last_triggered: Optional[datetime] = None
    enabled: bool = True
    description: str = ''

class LogManager:
    """æ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, project_root: str = None):
        """åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨"""
        if project_root is None:
            self.project_root = Path(__file__).parent.parent.parent
        else:
            self.project_root = Path(project_root)
        
        self.config_dir = self.project_root / "Struc" / "GeneralOffice" / "config"
        self.logs_dir = self.project_root / "logs"
        self.archive_dir = self.logs_dir / "archive"
        
        # åˆ›å»ºç›®å½•
        for directory in [self.logs_dir, self.archive_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½é…ç½®
        self.config = self._load_config()
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.loggers = {}
        self.handlers = {}
        self.alerts = {}
        self.monitoring_thread = None
        self.monitoring_active = False
        
        # è®¾ç½®ä¸»æ—¥å¿—è®°å½•å™¨
        self._setup_main_logger()
        
        # åŠ è½½å‘Šè­¦è§„åˆ™
        self._load_alerts()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½æ—¥å¿—é…ç½®"""
        config_file = self.config_dir / "logging_config.yaml"
        
        default_config = {
            'logging': {
                'version': 1,
                'disable_existing_loggers': False,
                'formatters': {
                    'standard': {
                        'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                        'datefmt': '%Y-%m-%d %H:%M:%S'
                    },
                    'detailed': {
                        'format': '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(funcName)s - %(message)s',
                        'datefmt': '%Y-%m-%d %H:%M:%S'
                    },
                    'json': {
                        'format': '{"timestamp": "%(asctime)s", "logger": "%(name)s", "level": "%(levelname)s", "message": "%(message)s", "file": "%(filename)s", "line": %(lineno)d, "function": "%(funcName)s"}',
                        'datefmt': '%Y-%m-%d %H:%M:%S'
                    },
                    'colored': {
                        'format': '%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s%(reset)s',
                        'datefmt': '%Y-%m-%d %H:%M:%S'
                    }
                },
                'handlers': {
                    'console': {
                        'class': 'logging.StreamHandler',
                        'level': 'INFO',
                        'formatter': 'colored' if COLORLOG_AVAILABLE else 'standard',
                        'stream': 'ext://sys.stdout'
                    },
                    'file': {
                        'class': 'logging.handlers.RotatingFileHandler',
                        'level': 'DEBUG',
                        'formatter': 'detailed',
                        'filename': str(self.logs_dir / 'yds_lab.log'),
                        'maxBytes': 10485760,  # 10MB
                        'backupCount': 5,
                        'encoding': 'utf-8'
                    },
                    'error_file': {
                        'class': 'logging.handlers.RotatingFileHandler',
                        'level': 'ERROR',
                        'formatter': 'detailed',
                        'filename': str(self.logs_dir / 'error.log'),
                        'maxBytes': 10485760,  # 10MB
                        'backupCount': 10,
                        'encoding': 'utf-8'
                    },
                    'json_file': {
                        'class': 'logging.handlers.TimedRotatingFileHandler',
                        'level': 'INFO',
                        'formatter': 'json',
                        'filename': str(self.logs_dir / 'yds_lab.json'),
                        'when': 'midnight',
                        'interval': 1,
                        'backupCount': 30,
                        'encoding': 'utf-8'
                    }
                },
                'loggers': {
                    'yds_lab': {
                        'level': 'DEBUG',
                        'handlers': ['console', 'file', 'error_file', 'json_file'],
                        'propagate': False
                    }
                },
                'root': {
                    'level': 'WARNING',
                    'handlers': ['console']
                }
            },
            'management': {
                'max_log_age_days': 30,
                'max_archive_age_days': 90,
                'max_total_size_mb': 1000,
                'compression_enabled': True,
                'auto_cleanup_enabled': True,
                'cleanup_interval_hours': 24
            },
            'analysis': {
                'error_patterns': [
                    r'ERROR',
                    r'CRITICAL',
                    r'Exception',
                    r'Traceback',
                    r'Failed',
                    r'Error:',
                    r'timeout',
                    r'connection.*failed',
                    r'permission.*denied'
                ],
                'warning_patterns': [
                    r'WARNING',
                    r'WARN',
                    r'deprecated',
                    r'slow.*query',
                    r'memory.*usage',
                    r'disk.*space'
                ],
                'performance_patterns': [
                    r'slow.*response',
                    r'high.*cpu',
                    r'memory.*leak',
                    r'database.*slow',
                    r'timeout.*exceeded'
                ]
            },
            'alerts': {
                'enabled': True,
                'check_interval_minutes': 5,
                'email_notifications': False,
                'webhook_notifications': False,
                'default_rules': [
                    {
                        'id': 'high_error_rate',
                        'level': 'ERROR',
                        'pattern': r'ERROR|CRITICAL',
                        'threshold': 10,
                        'time_window': 60,
                        'description': 'é”™è¯¯ç‡è¿‡é«˜'
                    },
                    {
                        'id': 'memory_warning',
                        'level': 'WARNING',
                        'pattern': r'memory.*usage|memory.*leak',
                        'threshold': 5,
                        'time_window': 30,
                        'description': 'å†…å­˜ä½¿ç”¨è­¦å‘Š'
                    },
                    {
                        'id': 'database_errors',
                        'level': 'ERROR',
                        'pattern': r'database.*error|connection.*failed',
                        'threshold': 3,
                        'time_window': 15,
                        'description': 'æ•°æ®åº“è¿æ¥é”™è¯¯'
                    }
                ]
            },
            'monitoring': {
                'enabled': True,
                'watch_directories': [
                    str(self.logs_dir)
                ],
                'file_patterns': ['*.log', '*.json'],
                'real_time_analysis': True,
                'performance_tracking': True
            }
        }
        
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    # æ·±åº¦åˆå¹¶é…ç½®
                    self._deep_merge_config(default_config, user_config)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ—¥å¿—é…ç½®å¤±è´¥: {e}")
        
        return default_config
    
    def _deep_merge_config(self, base: Dict, update: Dict):
        """æ·±åº¦åˆå¹¶é…ç½®"""
        for key, value in update.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                self._deep_merge_config(base[key], value)
            else:
                base[key] = value
    
    def _setup_main_logger(self):
        """è®¾ç½®ä¸»æ—¥å¿—è®°å½•å™¨"""
        # é…ç½®æ—¥å¿—ç³»ç»Ÿ
        logging.config.dictConfig(self.config['logging'])
        
        # è·å–ä¸»è®°å½•å™¨
        self.main_logger = logging.getLogger('yds_lab.log_manager')
        
        # å¦‚æœå¯ç”¨äº†å½©è‰²æ—¥å¿—
        if COLORLOG_AVAILABLE:
            console_handler = None
            for handler in self.main_logger.handlers:
                if isinstance(handler, logging.StreamHandler) and handler.stream == sys.stdout:
                    console_handler = handler
                    break
            
            if console_handler:
                colored_formatter = colorlog.ColoredFormatter(
                    '%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s%(reset)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    log_colors={
                        'DEBUG': 'cyan',
                        'INFO': 'green',
                        'WARNING': 'yellow',
                        'ERROR': 'red',
                        'CRITICAL': 'red,bg_white',
                    }
                )
                console_handler.setFormatter(colored_formatter)
    
    def _load_alerts(self):
        """åŠ è½½å‘Šè­¦è§„åˆ™"""
        if not self.config['alerts']['enabled']:
            return
        
        for rule_config in self.config['alerts']['default_rules']:
            alert = LogAlert(
                id=rule_config['id'],
                level=rule_config['level'],
                pattern=rule_config['pattern'],
                threshold=rule_config['threshold'],
                time_window=rule_config['time_window'],
                description=rule_config.get('description', '')
            )
            self.alerts[alert.id] = alert
    
    def get_logger(self, name: str, level: str = 'INFO', 
                   handlers: List[str] = None) -> logging.Logger:
        """è·å–æˆ–åˆ›å»ºæ—¥å¿—è®°å½•å™¨"""
        if name in self.loggers:
            return self.loggers[name]
        
        logger = logging.getLogger(f'yds_lab.{name}')
        logger.setLevel(getattr(logging, level.upper()))
        
        # æ·»åŠ å¤„ç†å™¨
        if handlers is None:
            handlers = ['console', 'file']
        
        for handler_name in handlers:
            if handler_name in self.config['logging']['handlers']:
                handler_config = self.config['logging']['handlers'][handler_name]
                handler = self._create_handler(handler_config, name)
                if handler:
                    logger.addHandler(handler)
        
        self.loggers[name] = logger
        return logger
    
    def _create_handler(self, config: Dict[str, Any], logger_name: str) -> Optional[logging.Handler]:
        """åˆ›å»ºæ—¥å¿—å¤„ç†å™¨"""
        try:
            handler_class = config['class']
            
            if handler_class == 'logging.StreamHandler':
                handler = logging.StreamHandler()
            elif handler_class == 'logging.handlers.RotatingFileHandler':
                # ä¸ºä¸åŒçš„è®°å½•å™¨åˆ›å»ºä¸åŒçš„æ–‡ä»¶
                filename = config['filename']
                if logger_name != 'yds_lab':
                    base_path = Path(filename)
                    filename = str(base_path.parent / f"{logger_name}_{base_path.name}")
                
                handler = logging_handlers.RotatingFileHandler(
                    filename=filename,
                    maxBytes=config.get('maxBytes', 10485760),
                    backupCount=config.get('backupCount', 5),
                    encoding=config.get('encoding', 'utf-8')
                )
            elif handler_class == 'logging.handlers.TimedRotatingFileHandler':
                # ä¸ºä¸åŒçš„è®°å½•å™¨åˆ›å»ºä¸åŒçš„æ–‡ä»¶
                filename = config['filename']
                if logger_name != 'yds_lab':
                    base_path = Path(filename)
                    filename = str(base_path.parent / f"{logger_name}_{base_path.name}")
                
                handler = logging_handlers.TimedRotatingFileHandler(
                    filename=filename,
                    when=config.get('when', 'midnight'),
                    interval=config.get('interval', 1),
                    backupCount=config.get('backupCount', 30),
                    encoding=config.get('encoding', 'utf-8')
                )
            else:
                self.main_logger.warning(f"ä¸æ”¯æŒçš„å¤„ç†å™¨ç±»å‹: {handler_class}")
                return None
            
            # è®¾ç½®çº§åˆ«
            handler.setLevel(getattr(logging, config.get('level', 'INFO')))
            
            # è®¾ç½®æ ¼å¼å™¨
            formatter_name = config.get('formatter', 'standard')
            if formatter_name in self.config['logging']['formatters']:
                formatter_config = self.config['logging']['formatters'][formatter_name]
                
                if formatter_name == 'colored' and COLORLOG_AVAILABLE:
                    formatter = colorlog.ColoredFormatter(
                        formatter_config['format'],
                        datefmt=formatter_config.get('datefmt'),
                        log_colors={
                            'DEBUG': 'cyan',
                            'INFO': 'green',
                            'WARNING': 'yellow',
                            'ERROR': 'red',
                            'CRITICAL': 'red,bg_white',
                        }
                    )
                else:
                    formatter = logging.Formatter(
                        formatter_config['format'],
                        datefmt=formatter_config.get('datefmt')
                    )
                
                handler.setFormatter(formatter)
            
            return handler
        
        except Exception as e:
            self.main_logger.error(f"åˆ›å»ºå¤„ç†å™¨å¤±è´¥: {e}")
            return None
    
    def analyze_logs(self, log_files: List[str] = None, 
                    start_time: datetime = None, 
                    end_time: datetime = None) -> LogStats:
        """åˆ†ææ—¥å¿—æ–‡ä»¶"""
        if log_files is None:
            log_files = list(self.logs_dir.glob('*.log'))
        else:
            log_files = [Path(f) for f in log_files]
        
        stats = LogStats()
        entries = []
        
        self.main_logger.info(f"å¼€å§‹åˆ†æ {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        
        for log_file in log_files:
            if not log_file.exists():
                continue
            
            try:
                file_entries = self._parse_log_file(log_file, start_time, end_time)
                entries.extend(file_entries)
                stats.file_sizes[str(log_file)] = log_file.stat().st_size
            except Exception as e:
                self.main_logger.error(f"åˆ†ææ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
        
        # ç»Ÿè®¡åˆ†æ
        stats.total_entries = len(entries)
        
        if entries:
            # çº§åˆ«ç»Ÿè®¡
            stats.level_counts = Counter(entry.level for entry in entries)
            
            # è®°å½•å™¨ç»Ÿè®¡
            stats.logger_counts = Counter(entry.logger for entry in entries)
            
            # æ—¶é—´èŒƒå›´
            timestamps = [entry.timestamp for entry in entries]
            stats.time_range = (min(timestamps), max(timestamps))
            
            # é”™è¯¯æ¨¡å¼åˆ†æ
            stats.error_patterns = self._analyze_error_patterns(entries)
        
        self.main_logger.info(f"æ—¥å¿—åˆ†æå®Œæˆï¼Œå…±å¤„ç† {stats.total_entries} æ¡è®°å½•")
        return stats
    
    def _parse_log_file(self, log_file: Path, start_time: datetime = None, 
                       end_time: datetime = None) -> List[LogEntry]:
        """è§£ææ—¥å¿—æ–‡ä»¶"""
        entries = []
        
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        # å°è¯•è§£æJSONæ ¼å¼
                        if line.startswith('{') and line.endswith('}'):
                            entry = self._parse_json_log_line(line)
                        else:
                            # è§£ææ ‡å‡†æ ¼å¼
                            entry = self._parse_standard_log_line(line)
                        
                        if entry:
                            # æ—¶é—´è¿‡æ»¤
                            if start_time and entry.timestamp < start_time:
                                continue
                            if end_time and entry.timestamp > end_time:
                                continue
                            
                            entries.append(entry)
                    
                    except Exception as e:
                        self.main_logger.debug(f"è§£ææ—¥å¿—è¡Œå¤±è´¥ {log_file}:{line_num}: {e}")
        
        except Exception as e:
            self.main_logger.error(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
        
        return entries
    
    def _parse_json_log_line(self, line: str) -> Optional[LogEntry]:
        """è§£æJSONæ ¼å¼æ—¥å¿—è¡Œ"""
        try:
            data = json.loads(line)
            
            timestamp = datetime.strptime(data['timestamp'], '%Y-%m-%d %H:%M:%S')
            
            entry = LogEntry(
                timestamp=timestamp,
                level=data.get('level', 'INFO'),
                logger=data.get('logger', ''),
                message=data.get('message', ''),
                file_path=data.get('file', ''),
                line_number=data.get('line', 0),
                function=data.get('function', ''),
                extra_data=data
            )
            
            return entry
        
        except Exception:
            return None
    
    def _parse_standard_log_line(self, line: str) -> Optional[LogEntry]:
        """è§£ææ ‡å‡†æ ¼å¼æ—¥å¿—è¡Œ"""
        # æ ‡å‡†æ ¼å¼: 2024-01-01 12:00:00 - logger - LEVEL - message
        pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) - ([^-]+) - ([^-]+) - (.+)'
        
        match = re.match(pattern, line)
        if not match:
            return None
        
        try:
            timestamp_str, logger, level, message = match.groups()
            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
            
            entry = LogEntry(
                timestamp=timestamp,
                level=level.strip(),
                logger=logger.strip(),
                message=message.strip()
            )
            
            return entry
        
        except Exception:
            return None
    
    def _analyze_error_patterns(self, entries: List[LogEntry]) -> Dict[str, int]:
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        error_patterns = {}
        
        all_patterns = (
            self.config['analysis']['error_patterns'] +
            self.config['analysis']['warning_patterns'] +
            self.config['analysis']['performance_patterns']
        )
        
        for pattern_str in all_patterns:
            pattern = re.compile(pattern_str, re.IGNORECASE)
            count = 0
            
            for entry in entries:
                if pattern.search(entry.message):
                    count += 1
            
            if count > 0:
                error_patterns[pattern_str] = count
        
        return error_patterns
    
    def search_logs(self, query: str, log_files: List[str] = None,
                   start_time: datetime = None, end_time: datetime = None,
                   level: str = None, logger: str = None) -> List[LogEntry]:
        """æœç´¢æ—¥å¿—"""
        if log_files is None:
            log_files = list(self.logs_dir.glob('*.log'))
        else:
            log_files = [Path(f) for f in log_files]
        
        results = []
        query_pattern = re.compile(query, re.IGNORECASE)
        
        for log_file in log_files:
            if not log_file.exists():
                continue
            
            try:
                entries = self._parse_log_file(log_file, start_time, end_time)
                
                for entry in entries:
                    # çº§åˆ«è¿‡æ»¤
                    if level and entry.level != level:
                        continue
                    
                    # è®°å½•å™¨è¿‡æ»¤
                    if logger and entry.logger != logger:
                        continue
                    
                    # æŸ¥è¯¢åŒ¹é…
                    if query_pattern.search(entry.message):
                        results.append(entry)
            
            except Exception as e:
                self.main_logger.error(f"æœç´¢æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
        
        return results
    
    def cleanup_logs(self, dry_run: bool = False) -> Dict[str, Any]:
        """æ¸…ç†æ—¥å¿—æ–‡ä»¶"""
        cleanup_result = {
            'deleted_files': [],
            'archived_files': [],
            'compressed_files': [],
            'total_space_freed': 0,
            'errors': []
        }
        
        max_age = timedelta(days=self.config['management']['max_log_age_days'])
        max_archive_age = timedelta(days=self.config['management']['max_archive_age_days'])
        max_total_size = self.config['management']['max_total_size_mb'] * 1024 * 1024
        compression_enabled = self.config['management']['compression_enabled']
        
        current_time = datetime.now()
        
        # è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        log_files = []
        for pattern in ['*.log', '*.log.*', '*.json', '*.json.*']:
            log_files.extend(self.logs_dir.glob(pattern))
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        log_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        
        total_size = 0
        
        for log_file in log_files:
            try:
                file_stat = log_file.stat()
                file_age = current_time - datetime.fromtimestamp(file_stat.st_mtime)
                file_size = file_stat.st_size
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤
                if file_age > max_age:
                    if not dry_run:
                        log_file.unlink()
                    cleanup_result['deleted_files'].append(str(log_file))
                    cleanup_result['total_space_freed'] += file_size
                    continue
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
                if (compression_enabled and 
                    not log_file.name.endswith('.gz') and 
                    file_age > timedelta(days=1) and
                    file_size > 1024 * 1024):  # 1MB
                    
                    if not dry_run:
                        compressed_file = self._compress_log_file(log_file)
                        if compressed_file:
                            cleanup_result['compressed_files'].append(str(compressed_file))
                            cleanup_result['total_space_freed'] += file_size - compressed_file.stat().st_size
                    else:
                        cleanup_result['compressed_files'].append(str(log_file) + '.gz')
                
                # æ£€æŸ¥æ€»å¤§å°é™åˆ¶
                total_size += file_size
                if total_size > max_total_size:
                    # åˆ é™¤æœ€æ—§çš„æ–‡ä»¶
                    if not dry_run:
                        log_file.unlink()
                    cleanup_result['deleted_files'].append(str(log_file))
                    cleanup_result['total_space_freed'] += file_size
            
            except Exception as e:
                error_msg = f"å¤„ç†æ–‡ä»¶å¤±è´¥ {log_file}: {e}"
                cleanup_result['errors'].append(error_msg)
                self.main_logger.error(error_msg)
        
        # æ¸…ç†å½’æ¡£ç›®å½•
        if self.archive_dir.exists():
            for archive_file in self.archive_dir.glob('*'):
                try:
                    file_stat = archive_file.stat()
                    file_age = current_time - datetime.fromtimestamp(file_stat.st_mtime)
                    
                    if file_age > max_archive_age:
                        if not dry_run:
                            archive_file.unlink()
                        cleanup_result['deleted_files'].append(str(archive_file))
                        cleanup_result['total_space_freed'] += file_stat.st_size
                
                except Exception as e:
                    error_msg = f"å¤„ç†å½’æ¡£æ–‡ä»¶å¤±è´¥ {archive_file}: {e}"
                    cleanup_result['errors'].append(error_msg)
                    self.main_logger.error(error_msg)
        
        self.main_logger.info(f"æ—¥å¿—æ¸…ç†å®Œæˆï¼Œé‡Šæ”¾ç©ºé—´: {cleanup_result['total_space_freed'] / 1024 / 1024:.2f} MB")
        return cleanup_result
    
    def _compress_log_file(self, log_file: Path) -> Optional[Path]:
        """å‹ç¼©æ—¥å¿—æ–‡ä»¶"""
        try:
            compressed_file = log_file.with_suffix(log_file.suffix + '.gz')
            
            with open(log_file, 'rb') as f_in:
                with gzip.open(compressed_file, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            # åˆ é™¤åŸæ–‡ä»¶
            log_file.unlink()
            
            return compressed_file
        
        except Exception as e:
            self.main_logger.error(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
            return None
    
    def start_monitoring(self):
        """å¯åŠ¨æ—¥å¿—ç›‘æ§"""
        if not self.config['monitoring']['enabled']:
            self.main_logger.info("æ—¥å¿—ç›‘æ§å·²ç¦ç”¨")
            return
        
        if self.monitoring_active:
            self.main_logger.warning("æ—¥å¿—ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitoring_thread.start()
        
        self.main_logger.info("æ—¥å¿—ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢æ—¥å¿—ç›‘æ§"""
        if not self.monitoring_active:
            return
        
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        
        self.main_logger.info("æ—¥å¿—ç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        check_interval = self.config['alerts']['check_interval_minutes'] * 60
        
        while self.monitoring_active:
            try:
                # æ£€æŸ¥å‘Šè­¦
                self._check_alerts()
                
                # è‡ªåŠ¨æ¸…ç†
                if self.config['management']['auto_cleanup_enabled']:
                    cleanup_interval = self.config['management']['cleanup_interval_hours'] * 3600
                    if hasattr(self, '_last_cleanup'):
                        if time.time() - self._last_cleanup > cleanup_interval:
                            self.cleanup_logs()
                            self._last_cleanup = time.time()
                    else:
                        self._last_cleanup = time.time()
                
                time.sleep(check_interval)
            
            except Exception as e:
                self.main_logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(60)  # å‡ºé”™æ—¶ç­‰å¾…1åˆ†é’Ÿ
    
    def _check_alerts(self):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        if not self.config['alerts']['enabled']:
            return
        
        current_time = datetime.now()
        
        for alert_id, alert in self.alerts.items():
            if not alert.enabled:
                continue
            
            try:
                # è·å–æ—¶é—´çª—å£å†…çš„æ—¥å¿—
                start_time = current_time - timedelta(minutes=alert.time_window)
                
                # æœç´¢åŒ¹é…çš„æ—¥å¿—æ¡ç›®
                matching_entries = self.search_logs(
                    query=alert.pattern,
                    start_time=start_time,
                    end_time=current_time,
                    level=alert.level if alert.level != 'ALL' else None
                )
                
                count = len(matching_entries)
                
                # æ£€æŸ¥æ˜¯å¦è§¦å‘å‘Šè­¦
                if count >= alert.threshold:
                    # æ£€æŸ¥å†·å´æ—¶é—´
                    if (alert.last_triggered is None or 
                        current_time - alert.last_triggered > timedelta(minutes=alert.time_window)):
                        
                        self._trigger_alert(alert, count, matching_entries)
                        alert.last_triggered = current_time
            
            except Exception as e:
                self.main_logger.error(f"æ£€æŸ¥å‘Šè­¦å¤±è´¥ {alert_id}: {e}")
    
    def _trigger_alert(self, alert: LogAlert, count: int, entries: List[LogEntry]):
        """è§¦å‘å‘Šè­¦"""
        message = f"å‘Šè­¦è§¦å‘: {alert.description} (ID: {alert.id})"
        message += f"\nåŒ¹é…æ¡ç›®æ•°: {count} (é˜ˆå€¼: {alert.threshold})"
        message += f"\næ—¶é—´çª—å£: {alert.time_window} åˆ†é’Ÿ"
        message += f"\næ¨¡å¼: {alert.pattern}"
        
        if entries:
            message += "\næœ€è¿‘çš„åŒ¹é…æ¡ç›®:"
            for entry in entries[-3:]:  # æ˜¾ç¤ºæœ€è¿‘3æ¡
                message += f"\n  {entry.timestamp} - {entry.level} - {entry.message[:100]}"
        
        self.main_logger.warning(message)
        
        # å‘é€é€šçŸ¥
        self._send_alert_notification(alert, message)
    
    def _send_alert_notification(self, alert: LogAlert, message: str):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        # è¿™é‡Œå¯ä»¥å®ç°é‚®ä»¶ã€Webhookç­‰é€šçŸ¥æ–¹å¼
        # ç›®å‰åªè®°å½•åˆ°æ—¥å¿—
        self.main_logger.critical(f"ALERT: {message}")
    
    def export_logs(self, output_file: str, format_type: str = 'json',
                   start_time: datetime = None, end_time: datetime = None,
                   level: str = None, logger: str = None) -> bool:
        """å¯¼å‡ºæ—¥å¿—"""
        try:
            # è·å–æ—¥å¿—æ¡ç›®
            entries = []
            for log_file in self.logs_dir.glob('*.log'):
                file_entries = self._parse_log_file(log_file, start_time, end_time)
                entries.extend(file_entries)
            
            # è¿‡æ»¤
            if level:
                entries = [e for e in entries if e.level == level]
            if logger:
                entries = [e for e in entries if e.logger == logger]
            
            # æ’åº
            entries.sort(key=lambda e: e.timestamp)
            
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            if format_type == 'json':
                data = [asdict(entry) for entry in entries]
                # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
                for item in data:
                    item['timestamp'] = item['timestamp'].isoformat()
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
            
            elif format_type == 'csv' and PANDAS_AVAILABLE:
                data = [asdict(entry) for entry in entries]
                df = pd.DataFrame(data)
                df.to_csv(output_path, index=False, encoding='utf-8')
            
            elif format_type == 'txt':
                with open(output_path, 'w', encoding='utf-8') as f:
                    for entry in entries:
                        f.write(f"{entry.timestamp} - {entry.logger} - {entry.level} - {entry.message}\n")
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format_type}")
            
            self.main_logger.info(f"æ—¥å¿—å¯¼å‡ºå®Œæˆ: {output_path}")
            return True
        
        except Exception as e:
            self.main_logger.error(f"å¯¼å‡ºæ—¥å¿—å¤±è´¥: {e}")
            return False
    
    def get_log_stats(self) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.analyze_logs()
        
        # æ–‡ä»¶ä¿¡æ¯
        log_files = list(self.logs_dir.glob('*.log'))
        total_size = sum(f.stat().st_size for f in log_files if f.exists())
        
        return {
            'total_entries': stats.total_entries,
            'level_counts': stats.level_counts,
            'logger_counts': stats.logger_counts,
            'error_patterns': stats.error_patterns,
            'time_range': stats.time_range,
            'file_count': len(log_files),
            'total_size_mb': total_size / 1024 / 1024,
            'file_sizes': {k: v / 1024 / 1024 for k, v in stats.file_sizes.items()},
            'alerts_count': len(self.alerts),
            'monitoring_active': self.monitoring_active
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='YDS-Lab æ—¥å¿—ç®¡ç†å·¥å…·')
    parser.add_argument('--project-root', help='æŒ‡å®šé¡¹ç›®æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--analyze', action='store_true', help='åˆ†ææ—¥å¿—æ–‡ä»¶')
    parser.add_argument('--search', help='æœç´¢æ—¥å¿—å†…å®¹')
    parser.add_argument('--cleanup', action='store_true', help='æ¸…ç†æ—¥å¿—æ–‡ä»¶')
    parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ‹Ÿè¿è¡Œï¼ˆä¸å®é™…åˆ é™¤æ–‡ä»¶ï¼‰')
    parser.add_argument('--export', help='å¯¼å‡ºæ—¥å¿—åˆ°æ–‡ä»¶')
    parser.add_argument('--format', choices=['json', 'csv', 'txt'], default='json', help='å¯¼å‡ºæ ¼å¼')
    parser.add_argument('--level', help='è¿‡æ»¤æ—¥å¿—çº§åˆ«')
    parser.add_argument('--logger', help='è¿‡æ»¤è®°å½•å™¨åç§°')
    parser.add_argument('--start-time', help='å¼€å§‹æ—¶é—´ (YYYY-MM-DD HH:MM:SS)')
    parser.add_argument('--end-time', help='ç»“æŸæ—¶é—´ (YYYY-MM-DD HH:MM:SS)')
    parser.add_argument('--monitor', action='store_true', help='å¯åŠ¨ç›‘æ§æ¨¡å¼')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºæ—¥å¿—ç»Ÿè®¡')
    
    args = parser.parse_args()
    
    manager = LogManager(args.project_root)
    
    # è§£ææ—¶é—´å‚æ•°
    start_time = None
    end_time = None
    
    if args.start_time:
        try:
            start_time = datetime.strptime(args.start_time, '%Y-%m-%d %H:%M:%S')
        except ValueError:
            print("âŒ å¼€å§‹æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD HH:MM:SS")
            return
    
    if args.end_time:
        try:
            end_time = datetime.strptime(args.end_time, '%Y-%m-%d %H:%M:%S')
        except ValueError:
            print("âŒ ç»“æŸæ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD HH:MM:SS")
            return
    
    # åˆ†ææ—¥å¿—
    if args.analyze:
        print("ğŸ“Š åˆ†ææ—¥å¿—æ–‡ä»¶")
        stats = manager.analyze_logs(start_time=start_time, end_time=end_time)
        
        print("="*50)
        print(f"æ€»æ¡ç›®æ•°: {stats.total_entries}")
        print(f"æ—¶é—´èŒƒå›´: {stats.time_range[0]} - {stats.time_range[1]}" if stats.time_range else "æ— æ•°æ®")
        
        print("\nçº§åˆ«ç»Ÿè®¡:")
        for level, count in stats.level_counts.items():
            print(f"  {level}: {count}")
        
        print("\nè®°å½•å™¨ç»Ÿè®¡:")
        for logger, count in sorted(stats.logger_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {logger}: {count}")
        
        print("\né”™è¯¯æ¨¡å¼:")
        for pattern, count in sorted(stats.error_patterns.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {pattern}: {count}")
        
        return
    
    # æœç´¢æ—¥å¿—
    if args.search:
        print(f"ğŸ” æœç´¢æ—¥å¿—: {args.search}")
        results = manager.search_logs(
            query=args.search,
            start_time=start_time,
            end_time=end_time,
            level=args.level,
            logger=args.logger
        )
        
        print(f"æ‰¾åˆ° {len(results)} æ¡åŒ¹é…è®°å½•:")
        for entry in results[-20:]:  # æ˜¾ç¤ºæœ€è¿‘20æ¡
            print(f"{entry.timestamp} - {entry.logger} - {entry.level} - {entry.message}")
        
        return
    
    # æ¸…ç†æ—¥å¿—
    if args.cleanup:
        print("ğŸ§¹ æ¸…ç†æ—¥å¿—æ–‡ä»¶")
        result = manager.cleanup_logs(dry_run=args.dry_run)
        
        print("="*50)
        print(f"åˆ é™¤æ–‡ä»¶: {len(result['deleted_files'])}")
        print(f"å‹ç¼©æ–‡ä»¶: {len(result['compressed_files'])}")
        print(f"é‡Šæ”¾ç©ºé—´: {result['total_space_freed'] / 1024 / 1024:.2f} MB")
        
        if result['errors']:
            print(f"é”™è¯¯: {len(result['errors'])}")
            for error in result['errors'][:5]:
                print(f"  {error}")
        
        return
    
    # å¯¼å‡ºæ—¥å¿—
    if args.export:
        print(f"ğŸ“¤ å¯¼å‡ºæ—¥å¿—åˆ°: {args.export}")
        success = manager.export_logs(
            output_file=args.export,
            format_type=args.format,
            start_time=start_time,
            end_time=end_time,
            level=args.level,
            logger=args.logger
        )
        
        if success:
            print("âœ… å¯¼å‡ºæˆåŠŸ")
        else:
            print("âŒ å¯¼å‡ºå¤±è´¥")
        
        return
    
    # æ˜¾ç¤ºç»Ÿè®¡
    if args.stats:
        print("ğŸ“ˆ æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯")
        stats = manager.get_log_stats()
        
        print("="*50)
        print(f"æ€»æ¡ç›®æ•°: {stats['total_entries']}")
        print(f"æ–‡ä»¶æ•°é‡: {stats['file_count']}")
        print(f"æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
        print(f"å‘Šè­¦è§„åˆ™: {stats['alerts_count']}")
        print(f"ç›‘æ§çŠ¶æ€: {'è¿è¡Œä¸­' if stats['monitoring_active'] else 'å·²åœæ­¢'}")
        
        print("\nçº§åˆ«åˆ†å¸ƒ:")
        for level, count in stats['level_counts'].items():
            print(f"  {level}: {count}")
        
        return
    
    # ç›‘æ§æ¨¡å¼
    if args.monitor:
        print("ğŸ‘ï¸ å¯åŠ¨æ—¥å¿—ç›‘æ§")
        manager.start_monitoring()
        
        try:
            print("ç›‘æ§è¿è¡Œä¸­ï¼ŒæŒ‰ Ctrl+C åœæ­¢...")
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nåœæ­¢ç›‘æ§...")
            manager.stop_monitoring()
        
        return
    
    # é»˜è®¤æ˜¾ç¤ºçŠ¶æ€
    print("ğŸ“‹ æ—¥å¿—ç®¡ç†å™¨çŠ¶æ€")
    print("="*40)
    print(f"é¡¹ç›®è·¯å¾„: {manager.project_root}")
    print(f"æ—¥å¿—ç›®å½•: {manager.logs_dir}")
    print(f"å½’æ¡£ç›®å½•: {manager.archive_dir}")
    
    stats = manager.get_log_stats()
    print(f"æ—¥å¿—æ–‡ä»¶: {stats['file_count']} ä¸ª")
    print(f"æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
    print(f"ç›‘æ§çŠ¶æ€: {'è¿è¡Œä¸­' if stats['monitoring_active'] else 'å·²åœæ­¢'}")

if __name__ == "__main__":
    main()