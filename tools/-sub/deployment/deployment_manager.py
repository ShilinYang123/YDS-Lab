#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YDS-Lab éƒ¨ç½²ç®¡ç†å·¥å…·
æä¾›è‡ªåŠ¨åŒ–éƒ¨ç½²ã€ç¯å¢ƒç®¡ç†ã€å‘å¸ƒæµç¨‹å’Œå›æ»šåŠŸèƒ½
é€‚é…YDS-Labé¡¹ç›®ç»“æ„å’ŒAI Agentåä½œéœ€æ±‚
"""

import os
import sys
import json
import yaml
import time
import logging
import shutil
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import threading
import hashlib
import zipfile
import tarfile

try:
    import docker
    DOCKER_AVAILABLE = True
except ImportError:
    DOCKER_AVAILABLE = False

try:
    import paramiko
    SSH_AVAILABLE = True
except ImportError:
    SSH_AVAILABLE = False

try:
    import boto3
    AWS_AVAILABLE = True
except ImportError:
    AWS_AVAILABLE = False

@dataclass
class DeploymentTarget:
    """éƒ¨ç½²ç›®æ ‡é…ç½®"""
    name: str
    type: str  # local, ssh, docker, k8s, aws, azure, gcp
    host: str = ''
    port: int = 22
    username: str = ''
    password: str = ''
    key_file: str = ''
    docker_image: str = ''
    k8s_namespace: str = 'default'
    aws_region: str = 'us-east-1'
    environment: str = 'production'
    config: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.config is None:
            self.config = {}

@dataclass
class DeploymentJob:
    """éƒ¨ç½²ä»»åŠ¡"""
    id: str
    name: str
    target: str
    status: str = 'pending'  # pending, running, success, failed, cancelled
    start_time: datetime = None
    end_time: datetime = None
    log_file: str = ''
    error: str = ''
    rollback_id: str = ''
    config: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.config is None:
            self.config = {}

@dataclass
class DeploymentHistory:
    """éƒ¨ç½²å†å²è®°å½•"""
    id: str
    job_id: str
    target: str
    version: str
    timestamp: datetime
    status: str
    duration: float = 0
    files_changed: int = 0
    rollback_available: bool = True
    backup_path: str = ''

class DeploymentManager:
    """éƒ¨ç½²ç®¡ç†å™¨"""
    
    def __init__(self, project_root: str = None):
        """åˆå§‹åŒ–éƒ¨ç½²ç®¡ç†å™¨"""
        if project_root is None:
            self.project_root = Path(__file__).parent.parent.parent
        else:
            self.project_root = Path(project_root)
        
        self.config_dir = self.project_root / "Struc" / "GeneralOffice" / "config"
        self.data_dir = self.project_root / "data" / "deployment"
        self.logs_dir = self.project_root / "logs" / "deployment"
        self.backups_dir = self.project_root / "backups" / "deployment"
        self.scripts_dir = self.project_root / "scripts" / "deployment"
        
        # åˆ›å»ºç›®å½•
        for directory in [self.data_dir, self.logs_dir, self.backups_dir, self.scripts_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½é…ç½®
        self.config = self._load_config()
        self.targets = self._load_targets()
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.jobs = {}
        self.history = []
        self.running_jobs = {}
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # åŠ è½½å†å²è®°å½•
        self._load_history()
        
        # åˆå§‹åŒ–Dockerå®¢æˆ·ç«¯
        self.docker_client = None
        if DOCKER_AVAILABLE:
            try:
                self.docker_client = docker.from_env()
            except Exception as e:
                self.logger.warning(f"Dockerå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½éƒ¨ç½²é…ç½®"""
        config_file = self.config_dir / "deployment_config.yaml"
        
        default_config = {
            'build': {
                'enabled': True,
                'command': 'npm run build',
                'output_dir': 'dist',
                'clean_before_build': True,
                'parallel_builds': False
            },
            'backup': {
                'enabled': True,
                'keep_backups': 5,
                'compress': True,
                'exclude_patterns': [
                    '*.log',
                    '*.tmp',
                    'node_modules',
                    '.git',
                    '__pycache__'
                ]
            },
            'deployment': {
                'timeout': 300,
                'retry_count': 3,
                'retry_delay': 10,
                'parallel_deployments': 2,
                'health_check': {
                    'enabled': True,
                    'url': '/health',
                    'timeout': 30,
                    'retries': 5
                },
                'rollback': {
                    'auto_rollback': True,
                    'rollback_timeout': 60
                }
            },
            'notifications': {
                'enabled': False,
                'webhook_url': '',
                'email': {
                    'enabled': False,
                    'smtp_server': '',
                    'smtp_port': 587,
                    'username': '',
                    'password': '',
                    'recipients': []
                },
                'slack': {
                    'enabled': False,
                    'webhook_url': '',
                    'channel': '#deployments'
                }
            },
            'security': {
                'verify_checksums': True,
                'encrypt_backups': False,
                'secure_transfer': True,
                'audit_log': True
            },
            'monitoring': {
                'enabled': True,
                'metrics_endpoint': '',
                'log_aggregation': False,
                'performance_tracking': True
            },
            'log_level': 'INFO'
        }
        
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    default_config.update(user_config)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½éƒ¨ç½²é…ç½®å¤±è´¥: {e}")
        
        return default_config
    
    def _load_targets(self) -> Dict[str, DeploymentTarget]:
        """åŠ è½½éƒ¨ç½²ç›®æ ‡é…ç½®"""
        targets = {}
        
        targets_file = self.config_dir / "deployment_targets.yaml"
        
        if targets_file.exists():
            try:
                with open(targets_file, 'r', encoding='utf-8') as f:
                    targets_config = yaml.safe_load(f)
                    
                    for name, config in targets_config.items():
                        targets[name] = DeploymentTarget(
                            name=name,
                            **config
                        )
            except Exception as e:
                print(f"âš ï¸ åŠ è½½éƒ¨ç½²ç›®æ ‡é…ç½®å¤±è´¥: {e}")
        
        return targets
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.logs_dir / f"deployment_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=getattr(logging, self.config['log_level']),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def _load_history(self):
        """åŠ è½½éƒ¨ç½²å†å²"""
        history_file = self.data_dir / "deployment_history.json"
        
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    history_data = json.load(f)
                    
                    for item in history_data:
                        # è½¬æ¢æ—¶é—´æˆ³
                        item['timestamp'] = datetime.fromisoformat(item['timestamp'])
                        
                        self.history.append(DeploymentHistory(**item))
            except Exception as e:
                self.logger.error(f"åŠ è½½éƒ¨ç½²å†å²å¤±è´¥: {e}")
    
    def _save_history(self):
        """ä¿å­˜éƒ¨ç½²å†å²"""
        history_file = self.data_dir / "deployment_history.json"
        
        try:
            history_data = []
            for item in self.history:
                data = asdict(item)
                data['timestamp'] = item.timestamp.isoformat()
                history_data.append(data)
            
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=2, ensure_ascii=False)
        
        except Exception as e:
            self.logger.error(f"ä¿å­˜éƒ¨ç½²å†å²å¤±è´¥: {e}")
    
    def create_deployment_job(self, name: str, target: str, config: Dict[str, Any] = None) -> str:
        """åˆ›å»ºéƒ¨ç½²ä»»åŠ¡"""
        job_id = f"deploy_{int(time.time())}_{hash(name) % 10000:04d}"
        
        job = DeploymentJob(
            id=job_id,
            name=name,
            target=target,
            config=config or {}
        )
        
        self.jobs[job_id] = job
        
        self.logger.info(f"éƒ¨ç½²ä»»åŠ¡å·²åˆ›å»º: {job_id} - {name}")
        return job_id
    
    def start_deployment(self, job_id: str) -> bool:
        """å¯åŠ¨éƒ¨ç½²ä»»åŠ¡"""
        if job_id not in self.jobs:
            self.logger.error(f"éƒ¨ç½²ä»»åŠ¡ä¸å­˜åœ¨: {job_id}")
            return False
        
        job = self.jobs[job_id]
        
        if job.status != 'pending':
            self.logger.error(f"éƒ¨ç½²ä»»åŠ¡çŠ¶æ€ä¸æ­£ç¡®: {job.status}")
            return False
        
        if job.target not in self.targets:
            self.logger.error(f"éƒ¨ç½²ç›®æ ‡ä¸å­˜åœ¨: {job.target}")
            return False
        
        # æ£€æŸ¥å¹¶å‘é™åˆ¶
        running_count = sum(1 for j in self.jobs.values() if j.status == 'running')
        if running_count >= self.config['deployment']['parallel_deployments']:
            self.logger.warning(f"è¾¾åˆ°å¹¶å‘éƒ¨ç½²é™åˆ¶: {running_count}")
            return False
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        job.status = 'running'
        job.start_time = datetime.now()
        job.log_file = str(self.logs_dir / f"deploy_{job_id}.log")
        
        # å¯åŠ¨éƒ¨ç½²çº¿ç¨‹
        thread = threading.Thread(
            target=self._run_deployment,
            args=(job_id,),
            daemon=True
        )
        thread.start()
        
        self.running_jobs[job_id] = thread
        
        self.logger.info(f"éƒ¨ç½²ä»»åŠ¡å·²å¯åŠ¨: {job_id}")
        return True
    
    def _run_deployment(self, job_id: str):
        """è¿è¡Œéƒ¨ç½²ä»»åŠ¡"""
        job = self.jobs[job_id]
        target = self.targets[job.target]
        
        try:
            # åˆ›å»ºå¤‡ä»½
            if self.config['backup']['enabled']:
                backup_id = self._create_backup(target)
                job.rollback_id = backup_id
            
            # æ„å»ºé¡¹ç›®
            if self.config['build']['enabled']:
                self._build_project(job)
            
            # æ‰§è¡Œéƒ¨ç½²
            success = self._deploy_to_target(job, target)
            
            if success:
                # å¥åº·æ£€æŸ¥
                if self.config['deployment']['health_check']['enabled']:
                    health_ok = self._health_check(target)
                    
                    if not health_ok and self.config['deployment']['rollback']['auto_rollback']:
                        self.logger.warning(f"å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå¼€å§‹è‡ªåŠ¨å›æ»š: {job_id}")
                        self._rollback_deployment(job_id)
                        success = False
                
                if success:
                    job.status = 'success'
                    self.logger.info(f"éƒ¨ç½²æˆåŠŸ: {job_id}")
                    
                    # è®°å½•å†å²
                    self._record_deployment_history(job, target, 'success')
                    
                    # å‘é€é€šçŸ¥
                    self._send_notification(job, 'success')
                else:
                    job.status = 'failed'
                    job.error = "å¥åº·æ£€æŸ¥å¤±è´¥"
            else:
                job.status = 'failed'
                self.logger.error(f"éƒ¨ç½²å¤±è´¥: {job_id}")
                
                # å‘é€é€šçŸ¥
                self._send_notification(job, 'failed')
        
        except Exception as e:
            job.status = 'failed'
            job.error = str(e)
            self.logger.error(f"éƒ¨ç½²å¼‚å¸¸: {job_id} - {e}")
            
            # å‘é€é€šçŸ¥
            self._send_notification(job, 'failed')
        
        finally:
            job.end_time = datetime.now()
            
            # æ¸…ç†è¿è¡ŒçŠ¶æ€
            if job_id in self.running_jobs:
                del self.running_jobs[job_id]
    
    def _build_project(self, job: DeploymentJob):
        """æ„å»ºé¡¹ç›®"""
        self.logger.info(f"å¼€å§‹æ„å»ºé¡¹ç›®: {job.id}")
        
        build_config = self.config['build']
        
        # æ¸…ç†æ„å»ºç›®å½•
        if build_config['clean_before_build']:
            output_dir = self.project_root / build_config['output_dir']
            if output_dir.exists():
                shutil.rmtree(output_dir)
        
        # æ‰§è¡Œæ„å»ºå‘½ä»¤
        try:
            result = subprocess.run(
                build_config['command'],
                shell=True,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=self.config['deployment']['timeout']
            )
            
            if result.returncode != 0:
                raise Exception(f"æ„å»ºå¤±è´¥: {result.stderr}")
            
            self.logger.info(f"é¡¹ç›®æ„å»ºå®Œæˆ: {job.id}")
        
        except subprocess.TimeoutExpired:
            raise Exception("æ„å»ºè¶…æ—¶")
        except Exception as e:
            raise Exception(f"æ„å»ºå¤±è´¥: {e}")
    
    def _deploy_to_target(self, job: DeploymentJob, target: DeploymentTarget) -> bool:
        """éƒ¨ç½²åˆ°ç›®æ ‡ç¯å¢ƒ"""
        self.logger.info(f"å¼€å§‹éƒ¨ç½²åˆ°ç›®æ ‡: {target.name}")
        
        try:
            if target.type == 'local':
                return self._deploy_local(job, target)
            elif target.type == 'ssh':
                return self._deploy_ssh(job, target)
            elif target.type == 'docker':
                return self._deploy_docker(job, target)
            elif target.type == 'k8s':
                return self._deploy_k8s(job, target)
            elif target.type == 'aws':
                return self._deploy_aws(job, target)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„éƒ¨ç½²ç±»å‹: {target.type}")
        
        except Exception as e:
            self.logger.error(f"éƒ¨ç½²å¤±è´¥: {e}")
            job.error = str(e)
            return False
    
    def _deploy_local(self, job: DeploymentJob, target: DeploymentTarget) -> bool:
        """æœ¬åœ°éƒ¨ç½²"""
        try:
            source_dir = self.project_root / self.config['build']['output_dir']
            target_dir = Path(target.config.get('target_dir', '/tmp/deployment'))
            
            # åˆ›å»ºç›®æ ‡ç›®å½•
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ–‡ä»¶
            if source_dir.exists():
                shutil.copytree(source_dir, target_dir, dirs_exist_ok=True)
            else:
                # å¤åˆ¶æ•´ä¸ªé¡¹ç›®
                for item in self.project_root.iterdir():
                    if item.name not in ['.git', 'node_modules', '__pycache__']:
                        if item.is_dir():
                            shutil.copytree(item, target_dir / item.name, dirs_exist_ok=True)
                        else:
                            shutil.copy2(item, target_dir)
            
            # æ‰§è¡Œéƒ¨ç½²åè„šæœ¬
            post_deploy_script = target.config.get('post_deploy_script')
            if post_deploy_script:
                subprocess.run(post_deploy_script, shell=True, cwd=target_dir, check=True)
            
            return True
        
        except Exception as e:
            self.logger.error(f"æœ¬åœ°éƒ¨ç½²å¤±è´¥: {e}")
            return False
    
    def _deploy_ssh(self, job: DeploymentJob, target: DeploymentTarget) -> bool:
        """SSHéƒ¨ç½²"""
        if not SSH_AVAILABLE:
            self.logger.error("paramikoåº“æœªå®‰è£…ï¼ŒSSHéƒ¨ç½²ä¸å¯ç”¨")
            return False
        
        try:
            # åˆ›å»ºSSHè¿æ¥
            ssh = paramiko.SSHClient()
            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            
            # è¿æ¥å‚æ•°
            connect_kwargs = {
                'hostname': target.host,
                'port': target.port,
                'username': target.username
            }
            
            if target.key_file:
                connect_kwargs['key_filename'] = target.key_file
            elif target.password:
                connect_kwargs['password'] = target.password
            
            ssh.connect(**connect_kwargs)
            
            # åˆ›å»ºSFTPè¿æ¥
            sftp = ssh.open_sftp()
            
            # ä¸Šä¼ æ–‡ä»¶
            source_dir = self.project_root / self.config['build']['output_dir']
            remote_dir = target.config.get('remote_dir', '/tmp/deployment')
            
            # åˆ›å»ºè¿œç¨‹ç›®å½•
            stdin, stdout, stderr = ssh.exec_command(f'mkdir -p {remote_dir}')
            stdout.channel.recv_exit_status()
            
            # é€’å½’ä¸Šä¼ æ–‡ä»¶
            self._upload_directory(sftp, source_dir, remote_dir)
            
            # æ‰§è¡Œéƒ¨ç½²è„šæœ¬
            deploy_script = target.config.get('deploy_script')
            if deploy_script:
                stdin, stdout, stderr = ssh.exec_command(f'cd {remote_dir} && {deploy_script}')
                exit_status = stdout.channel.recv_exit_status()
                
                if exit_status != 0:
                    error_output = stderr.read().decode()
                    raise Exception(f"éƒ¨ç½²è„šæœ¬æ‰§è¡Œå¤±è´¥: {error_output}")
            
            # å…³é—­è¿æ¥
            sftp.close()
            ssh.close()
            
            return True
        
        except Exception as e:
            self.logger.error(f"SSHéƒ¨ç½²å¤±è´¥: {e}")
            return False
    
    def _upload_directory(self, sftp, local_dir: Path, remote_dir: str):
        """é€’å½’ä¸Šä¼ ç›®å½•"""
        for item in local_dir.iterdir():
            local_path = str(item)
            remote_path = f"{remote_dir}/{item.name}"
            
            if item.is_dir():
                # åˆ›å»ºè¿œç¨‹ç›®å½•
                try:
                    sftp.mkdir(remote_path)
                except:
                    pass  # ç›®å½•å¯èƒ½å·²å­˜åœ¨
                
                # é€’å½’ä¸Šä¼ 
                self._upload_directory(sftp, item, remote_path)
            else:
                # ä¸Šä¼ æ–‡ä»¶
                sftp.put(local_path, remote_path)
    
    def _deploy_docker(self, job: DeploymentJob, target: DeploymentTarget) -> bool:
        """Dockeréƒ¨ç½²"""
        if not DOCKER_AVAILABLE or not self.docker_client:
            self.logger.error("Dockerä¸å¯ç”¨")
            return False
        
        try:
            # æ„å»ºDockeré•œåƒ
            image_name = target.docker_image or f"yds-lab-{job.name}:latest"
            
            dockerfile_path = self.project_root / "Dockerfile"
            if not dockerfile_path.exists():
                # åˆ›å»ºé»˜è®¤Dockerfile
                self._create_default_dockerfile()
            
            # æ„å»ºé•œåƒ
            self.logger.info(f"æ„å»ºDockeré•œåƒ: {image_name}")
            
            image, build_logs = self.docker_client.images.build(
                path=str(self.project_root),
                tag=image_name,
                rm=True
            )
            
            # åœæ­¢æ—§å®¹å™¨
            container_name = target.config.get('container_name', f"yds-lab-{job.name}")
            
            try:
                old_container = self.docker_client.containers.get(container_name)
                old_container.stop()
                old_container.remove()
            except docker.errors.NotFound:
                pass
            
            # å¯åŠ¨æ–°å®¹å™¨
            container_config = target.config.get('container_config', {})
            
            container = self.docker_client.containers.run(
                image_name,
                name=container_name,
                detach=True,
                **container_config
            )
            
            self.logger.info(f"Dockerå®¹å™¨å·²å¯åŠ¨: {container.id[:12]}")
            return True
        
        except Exception as e:
            self.logger.error(f"Dockeréƒ¨ç½²å¤±è´¥: {e}")
            return False
    
    def _create_default_dockerfile(self):
        """åˆ›å»ºé»˜è®¤Dockerfile"""
        dockerfile_content = """
FROM node:16-alpine

WORKDIR /app

COPY package*.json ./
RUN npm install

COPY . .
RUN npm run build

EXPOSE 3000

CMD ["npm", "start"]
"""
        
        dockerfile_path = self.project_root / "Dockerfile"
        with open(dockerfile_path, 'w', encoding='utf-8') as f:
            f.write(dockerfile_content.strip())
    
    def _deploy_k8s(self, job: DeploymentJob, target: DeploymentTarget) -> bool:
        """Kuberneteséƒ¨ç½²"""
        try:
            # è¿™é‡Œéœ€è¦kubectlå‘½ä»¤è¡Œå·¥å…·
            k8s_config = target.config
            namespace = target.k8s_namespace
            
            # åº”ç”¨Kubernetesé…ç½®
            manifest_file = k8s_config.get('manifest_file', 'k8s-deployment.yaml')
            manifest_path = self.project_root / manifest_file
            
            if not manifest_path.exists():
                self.logger.error(f"Kubernetesé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {manifest_file}")
                return False
            
            # æ‰§è¡Œkubectl apply
            cmd = f"kubectl apply -f {manifest_path} -n {namespace}"
            
            result = subprocess.run(
                cmd,
                shell=True,
                capture_output=True,
                text=True,
                timeout=self.config['deployment']['timeout']
            )
            
            if result.returncode != 0:
                raise Exception(f"kubectlæ‰§è¡Œå¤±è´¥: {result.stderr}")
            
            self.logger.info(f"Kuberneteséƒ¨ç½²å®Œæˆ: {namespace}")
            return True
        
        except Exception as e:
            self.logger.error(f"Kuberneteséƒ¨ç½²å¤±è´¥: {e}")
            return False
    
    def _deploy_aws(self, job: DeploymentJob, target: DeploymentTarget) -> bool:
        """AWSéƒ¨ç½²"""
        if not AWS_AVAILABLE:
            self.logger.error("boto3åº“æœªå®‰è£…ï¼ŒAWSéƒ¨ç½²ä¸å¯ç”¨")
            return False
        
        try:
            # è¿™é‡Œå¯ä»¥å®ç°AWS S3ã€EC2ã€ECSç­‰éƒ¨ç½²
            aws_config = target.config
            service_type = aws_config.get('service_type', 's3')
            
            if service_type == 's3':
                return self._deploy_aws_s3(job, target)
            elif service_type == 'ec2':
                return self._deploy_aws_ec2(job, target)
            elif service_type == 'ecs':
                return self._deploy_aws_ecs(job, target)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„AWSæœåŠ¡ç±»å‹: {service_type}")
        
        except Exception as e:
            self.logger.error(f"AWSéƒ¨ç½²å¤±è´¥: {e}")
            return False
    
    def _deploy_aws_s3(self, job: DeploymentJob, target: DeploymentTarget) -> bool:
        """AWS S3éƒ¨ç½²"""
        try:
            s3_client = boto3.client('s3', region_name=target.aws_region)
            
            bucket_name = target.config['bucket_name']
            source_dir = self.project_root / self.config['build']['output_dir']
            
            # ä¸Šä¼ æ–‡ä»¶åˆ°S3
            for file_path in source_dir.rglob('*'):
                if file_path.is_file():
                    relative_path = file_path.relative_to(source_dir)
                    s3_key = str(relative_path).replace('\\', '/')
                    
                    s3_client.upload_file(
                        str(file_path),
                        bucket_name,
                        s3_key
                    )
            
            self.logger.info(f"æ–‡ä»¶å·²ä¸Šä¼ åˆ°S3: {bucket_name}")
            return True
        
        except Exception as e:
            self.logger.error(f"S3éƒ¨ç½²å¤±è´¥: {e}")
            return False
    
    def _deploy_aws_ec2(self, job: DeploymentJob, target: DeploymentTarget) -> bool:
        """AWS EC2éƒ¨ç½²"""
        # å®ç°EC2éƒ¨ç½²é€»è¾‘
        pass
    
    def _deploy_aws_ecs(self, job: DeploymentJob, target: DeploymentTarget) -> bool:
        """AWS ECSéƒ¨ç½²"""
        # å®ç°ECSéƒ¨ç½²é€»è¾‘
        pass
    
    def _create_backup(self, target: DeploymentTarget) -> str:
        """åˆ›å»ºå¤‡ä»½"""
        backup_id = f"backup_{int(time.time())}_{target.name}"
        backup_dir = self.backups_dir / backup_id
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            if target.type == 'local':
                # æœ¬åœ°å¤‡ä»½
                target_dir = Path(target.config.get('target_dir', '/tmp/deployment'))
                if target_dir.exists():
                    shutil.copytree(target_dir, backup_dir / 'files')
            
            elif target.type == 'ssh':
                # SSHå¤‡ä»½
                # è¿™é‡Œå¯ä»¥å®ç°è¿œç¨‹å¤‡ä»½é€»è¾‘
                pass
            
            # å‹ç¼©å¤‡ä»½
            if self.config['backup']['compress']:
                backup_archive = f"{backup_dir}.tar.gz"
                
                with tarfile.open(backup_archive, 'w:gz') as tar:
                    tar.add(backup_dir, arcname=backup_id)
                
                # åˆ é™¤åŸç›®å½•
                shutil.rmtree(backup_dir)
            
            self.logger.info(f"å¤‡ä»½å·²åˆ›å»º: {backup_id}")
            return backup_id
        
        except Exception as e:
            self.logger.error(f"åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            return ""
    
    def _health_check(self, target: DeploymentTarget) -> bool:
        """å¥åº·æ£€æŸ¥"""
        health_config = self.config['deployment']['health_check']
        
        if not health_config['enabled']:
            return True
        
        try:
            import requests
            
            # æ„å»ºå¥åº·æ£€æŸ¥URL
            base_url = target.config.get('base_url', f"http://{target.host}")
            health_url = f"{base_url}{health_config['url']}"
            
            # é‡è¯•æ£€æŸ¥
            for i in range(health_config['retries']):
                try:
                    response = requests.get(
                        health_url,
                        timeout=health_config['timeout']
                    )
                    
                    if response.status_code == 200:
                        self.logger.info("å¥åº·æ£€æŸ¥é€šè¿‡")
                        return True
                
                except Exception as e:
                    self.logger.warning(f"å¥åº·æ£€æŸ¥å¤±è´¥ (å°è¯• {i+1}): {e}")
                
                if i < health_config['retries'] - 1:
                    time.sleep(5)
            
            return False
        
        except Exception as e:
            self.logger.error(f"å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    def _rollback_deployment(self, job_id: str) -> bool:
        """å›æ»šéƒ¨ç½²"""
        job = self.jobs[job_id]
        
        if not job.rollback_id:
            self.logger.error("æ²¡æœ‰å¯ç”¨çš„å›æ»šå¤‡ä»½")
            return False
        
        try:
            target = self.targets[job.target]
            
            # æ¢å¤å¤‡ä»½
            backup_path = self.backups_dir / f"{job.rollback_id}.tar.gz"
            
            if backup_path.exists():
                # è§£å‹å¤‡ä»½
                with tarfile.open(backup_path, 'r:gz') as tar:
                    tar.extractall(self.backups_dir)
                
                # æ¢å¤æ–‡ä»¶
                backup_dir = self.backups_dir / job.rollback_id / 'files'
                
                if target.type == 'local':
                    target_dir = Path(target.config.get('target_dir', '/tmp/deployment'))
                    
                    if target_dir.exists():
                        shutil.rmtree(target_dir)
                    
                    shutil.copytree(backup_dir, target_dir)
                
                self.logger.info(f"éƒ¨ç½²å·²å›æ»š: {job_id}")
                return True
            
            return False
        
        except Exception as e:
            self.logger.error(f"å›æ»šå¤±è´¥: {e}")
            return False
    
    def _record_deployment_history(self, job: DeploymentJob, target: DeploymentTarget, status: str):
        """è®°å½•éƒ¨ç½²å†å²"""
        duration = 0
        if job.start_time and job.end_time:
            duration = (job.end_time - job.start_time).total_seconds()
        
        history = DeploymentHistory(
            id=f"history_{int(time.time())}",
            job_id=job.id,
            target=target.name,
            version=job.config.get('version', 'latest'),
            timestamp=datetime.now(),
            status=status,
            duration=duration,
            rollback_available=bool(job.rollback_id),
            backup_path=job.rollback_id
        )
        
        self.history.append(history)
        self._save_history()
        
        # æ¸…ç†æ—§å†å²è®°å½•
        if len(self.history) > 100:
            self.history = self.history[-100:]
            self._save_history()
    
    def _send_notification(self, job: DeploymentJob, status: str):
        """å‘é€é€šçŸ¥"""
        if not self.config['notifications']['enabled']:
            return
        
        message = f"éƒ¨ç½²ä»»åŠ¡ {job.name} ({job.id}) çŠ¶æ€: {status}"
        
        if status == 'failed' and job.error:
            message += f"\né”™è¯¯: {job.error}"
        
        # Webhooké€šçŸ¥
        webhook_url = self.config['notifications']['webhook_url']
        if webhook_url:
            try:
                import requests
                
                payload = {
                    'text': message,
                    'job_id': job.id,
                    'status': status,
                    'target': job.target,
                    'timestamp': datetime.now().isoformat()
                }
                
                requests.post(webhook_url, json=payload, timeout=10)
            except Exception as e:
                self.logger.error(f"Webhooké€šçŸ¥å‘é€å¤±è´¥: {e}")
        
        # Slacké€šçŸ¥
        slack_config = self.config['notifications']['slack']
        if slack_config['enabled'] and slack_config['webhook_url']:
            try:
                import requests
                
                payload = {
                    'channel': slack_config['channel'],
                    'text': message,
                    'username': 'YDS-Lab Deployment Bot'
                }
                
                requests.post(slack_config['webhook_url'], json=payload, timeout=10)
            except Exception as e:
                self.logger.error(f"Slacké€šçŸ¥å‘é€å¤±è´¥: {e}")
    
    def cancel_deployment(self, job_id: str) -> bool:
        """å–æ¶ˆéƒ¨ç½²ä»»åŠ¡"""
        if job_id not in self.jobs:
            return False
        
        job = self.jobs[job_id]
        
        if job.status != 'running':
            return False
        
        job.status = 'cancelled'
        job.end_time = datetime.now()
        
        # åœæ­¢çº¿ç¨‹ï¼ˆæ³¨æ„ï¼šè¿™é‡Œåªæ˜¯æ ‡è®°ï¼Œå®é™…åœæ­¢éœ€è¦åœ¨éƒ¨ç½²é€»è¾‘ä¸­æ£€æŸ¥çŠ¶æ€ï¼‰
        if job_id in self.running_jobs:
            del self.running_jobs[job_id]
        
        self.logger.info(f"éƒ¨ç½²ä»»åŠ¡å·²å–æ¶ˆ: {job_id}")
        return True
    
    def get_job_status(self, job_id: str) -> Optional[DeploymentJob]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.jobs.get(job_id)
    
    def list_jobs(self, status: str = None) -> List[DeploymentJob]:
        """åˆ—å‡ºéƒ¨ç½²ä»»åŠ¡"""
        jobs = list(self.jobs.values())
        
        if status:
            jobs = [job for job in jobs if job.status == status]
        
        return sorted(jobs, key=lambda x: x.start_time or datetime.min, reverse=True)
    
    def list_targets(self) -> List[DeploymentTarget]:
        """åˆ—å‡ºéƒ¨ç½²ç›®æ ‡"""
        return list(self.targets.values())
    
    def add_target(self, target: DeploymentTarget) -> bool:
        """æ·»åŠ éƒ¨ç½²ç›®æ ‡"""
        try:
            self.targets[target.name] = target
            self._save_targets()
            
            self.logger.info(f"éƒ¨ç½²ç›®æ ‡å·²æ·»åŠ : {target.name}")
            return True
        
        except Exception as e:
            self.logger.error(f"æ·»åŠ éƒ¨ç½²ç›®æ ‡å¤±è´¥: {e}")
            return False
    
    def remove_target(self, name: str) -> bool:
        """ç§»é™¤éƒ¨ç½²ç›®æ ‡"""
        try:
            if name in self.targets:
                del self.targets[name]
                self._save_targets()
                
                self.logger.info(f"éƒ¨ç½²ç›®æ ‡å·²ç§»é™¤: {name}")
                return True
            else:
                return False
        
        except Exception as e:
            self.logger.error(f"ç§»é™¤éƒ¨ç½²ç›®æ ‡å¤±è´¥: {e}")
            return False
    
    def _save_targets(self):
        """ä¿å­˜éƒ¨ç½²ç›®æ ‡é…ç½®"""
        targets_file = self.config_dir / "deployment_targets.yaml"
        
        try:
            targets_config = {}
            for name, target in self.targets.items():
                config_dict = asdict(target)
                del config_dict['name']
                targets_config[name] = config_dict
            
            with open(targets_file, 'w', encoding='utf-8') as f:
                yaml.dump(targets_config, f, default_flow_style=False, allow_unicode=True)
        
        except Exception as e:
            self.logger.error(f"ä¿å­˜éƒ¨ç½²ç›®æ ‡é…ç½®å¤±è´¥: {e}")
    
    def get_deployment_history(self, target: str = None, limit: int = 50) -> List[DeploymentHistory]:
        """è·å–éƒ¨ç½²å†å²"""
        history = self.history
        
        if target:
            history = [h for h in history if h.target == target]
        
        return sorted(history, key=lambda x: x.timestamp, reverse=True)[:limit]
    
    def cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        try:
            keep_backups = self.config['backup']['keep_backups']
            
            # è·å–æ‰€æœ‰å¤‡ä»½æ–‡ä»¶
            backup_files = []
            for file_path in self.backups_dir.glob('backup_*.tar.gz'):
                backup_files.append((file_path.stat().st_mtime, file_path))
            
            # æŒ‰æ—¶é—´æ’åº
            backup_files.sort(reverse=True)
            
            # åˆ é™¤å¤šä½™çš„å¤‡ä»½
            for _, file_path in backup_files[keep_backups:]:
                file_path.unlink()
                self.logger.info(f"å·²åˆ é™¤æ—§å¤‡ä»½: {file_path.name}")
        
        except Exception as e:
            self.logger.error(f"æ¸…ç†å¤‡ä»½å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='YDS-Lab éƒ¨ç½²ç®¡ç†å·¥å…·')
    parser.add_argument('--project-root', help='æŒ‡å®šé¡¹ç›®æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--list-targets', action='store_true', help='åˆ—å‡ºéƒ¨ç½²ç›®æ ‡')
    parser.add_argument('--list-jobs', help='åˆ—å‡ºéƒ¨ç½²ä»»åŠ¡ (å¯é€‰çŠ¶æ€è¿‡æ»¤)')
    parser.add_argument('--deploy', nargs=2, metavar=('NAME', 'TARGET'), help='åˆ›å»ºå¹¶å¯åŠ¨éƒ¨ç½²ä»»åŠ¡')
    parser.add_argument('--status', help='æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€')
    parser.add_argument('--cancel', help='å–æ¶ˆéƒ¨ç½²ä»»åŠ¡')
    parser.add_argument('--rollback', help='å›æ»šéƒ¨ç½²')
    parser.add_argument('--history', help='æŸ¥çœ‹éƒ¨ç½²å†å² (å¯é€‰ç›®æ ‡è¿‡æ»¤)')
    parser.add_argument('--cleanup', action='store_true', help='æ¸…ç†æ—§å¤‡ä»½')
    
    args = parser.parse_args()
    
    manager = DeploymentManager(args.project_root)
    
    # åˆ—å‡ºéƒ¨ç½²ç›®æ ‡
    if args.list_targets:
        targets = manager.list_targets()
        
        print("ğŸ¯ éƒ¨ç½²ç›®æ ‡åˆ—è¡¨")
        print("="*50)
        
        for target in targets:
            print(f"åç§°: {target.name}")
            print(f"ç±»å‹: {target.type}")
            print(f"ä¸»æœº: {target.host}")
            print(f"ç¯å¢ƒ: {target.environment}")
            print("-" * 30)
        
        return
    
    # åˆ—å‡ºéƒ¨ç½²ä»»åŠ¡
    if args.list_jobs is not None:
        status_filter = args.list_jobs if args.list_jobs else None
        jobs = manager.list_jobs(status_filter)
        
        print("ğŸ“‹ éƒ¨ç½²ä»»åŠ¡åˆ—è¡¨")
        print("="*50)
        
        for job in jobs:
            print(f"ID: {job.id}")
            print(f"åç§°: {job.name}")
            print(f"ç›®æ ‡: {job.target}")
            print(f"çŠ¶æ€: {job.status}")
            
            if job.start_time:
                print(f"å¼€å§‹æ—¶é—´: {job.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            if job.end_time:
                duration = (job.end_time - job.start_time).total_seconds()
                print(f"æŒç»­æ—¶é—´: {duration:.1f}s")
            
            if job.error:
                print(f"é”™è¯¯: {job.error}")
            
            print("-" * 30)
        
        return
    
    # åˆ›å»ºå¹¶å¯åŠ¨éƒ¨ç½²
    if args.deploy:
        name, target = args.deploy
        
        print(f"ğŸš€ åˆ›å»ºéƒ¨ç½²ä»»åŠ¡: {name} -> {target}")
        
        job_id = manager.create_deployment_job(name, target)
        
        if manager.start_deployment(job_id):
            print(f"âœ… éƒ¨ç½²ä»»åŠ¡å·²å¯åŠ¨: {job_id}")
        else:
            print(f"âŒ éƒ¨ç½²ä»»åŠ¡å¯åŠ¨å¤±è´¥")
        
        return
    
    # æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
    if args.status:
        job = manager.get_job_status(args.status)
        
        if job:
            print(f"ğŸ“Š ä»»åŠ¡çŠ¶æ€: {args.status}")
            print("="*40)
            print(f"åç§°: {job.name}")
            print(f"ç›®æ ‡: {job.target}")
            print(f"çŠ¶æ€: {job.status}")
            
            if job.start_time:
                print(f"å¼€å§‹æ—¶é—´: {job.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            if job.end_time:
                print(f"ç»“æŸæ—¶é—´: {job.end_time.strftime('%Y-%m-%d %H:%M:%S')}")
                duration = (job.end_time - job.start_time).total_seconds()
                print(f"æŒç»­æ—¶é—´: {duration:.1f}s")
            
            if job.error:
                print(f"é”™è¯¯: {job.error}")
            
            if job.log_file and Path(job.log_file).exists():
                print(f"æ—¥å¿—æ–‡ä»¶: {job.log_file}")
        else:
            print(f"âŒ ä»»åŠ¡ä¸å­˜åœ¨: {args.status}")
        
        return
    
    # å–æ¶ˆéƒ¨ç½²
    if args.cancel:
        if manager.cancel_deployment(args.cancel):
            print(f"âœ… éƒ¨ç½²ä»»åŠ¡å·²å–æ¶ˆ: {args.cancel}")
        else:
            print(f"âŒ å–æ¶ˆéƒ¨ç½²å¤±è´¥: {args.cancel}")
        
        return
    
    # å›æ»šéƒ¨ç½²
    if args.rollback:
        if manager._rollback_deployment(args.rollback):
            print(f"âœ… éƒ¨ç½²å·²å›æ»š: {args.rollback}")
        else:
            print(f"âŒ å›æ»šå¤±è´¥: {args.rollback}")
        
        return
    
    # æŸ¥çœ‹éƒ¨ç½²å†å²
    if args.history is not None:
        target_filter = args.history if args.history else None
        history = manager.get_deployment_history(target_filter)
        
        print("ğŸ“š éƒ¨ç½²å†å²")
        print("="*50)
        
        for record in history:
            print(f"ID: {record.id}")
            print(f"ä»»åŠ¡: {record.job_id}")
            print(f"ç›®æ ‡: {record.target}")
            print(f"ç‰ˆæœ¬: {record.version}")
            print(f"æ—¶é—´: {record.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"çŠ¶æ€: {record.status}")
            print(f"æŒç»­æ—¶é—´: {record.duration:.1f}s")
            print(f"å¯å›æ»š: {'æ˜¯' if record.rollback_available else 'å¦'}")
            print("-" * 30)
        
        return
    
    # æ¸…ç†å¤‡ä»½
    if args.cleanup:
        print("ğŸ§¹ æ¸…ç†æ—§å¤‡ä»½")
        manager.cleanup_old_backups()
        print("âœ… æ¸…ç†å®Œæˆ")
        return
    
    # é»˜è®¤æ˜¾ç¤ºçŠ¶æ€
    print("ğŸš€ éƒ¨ç½²ç®¡ç†å™¨çŠ¶æ€")
    print("="*40)
    print(f"é¡¹ç›®è·¯å¾„: {manager.project_root}")
    print(f"éƒ¨ç½²ç›®æ ‡: {len(manager.targets)}")
    print(f"æ´»è·ƒä»»åŠ¡: {len([j for j in manager.jobs.values() if j.status == 'running'])}")
    print(f"å†å²è®°å½•: {len(manager.history)}")

if __name__ == "__main__":
    main()