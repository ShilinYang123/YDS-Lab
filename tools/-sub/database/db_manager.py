#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YDS-Lab æ•°æ®åº“ç®¡ç†å·¥å…·
æä¾›æ•°æ®åº“è¿æ¥ã€æ“ä½œã€å¤‡ä»½å’Œç»´æŠ¤åŠŸèƒ½
é€‚é…YDS-Labé¡¹ç›®ç»“æ„å’ŒAI Agentåä½œéœ€æ±‚
"""

import os
import sys
import json
import yaml
import sqlite3
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from contextlib import contextmanager
import threading
import time
import shutil
import subprocess

try:
    import pymongo
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False

try:
    import psycopg2
    import psycopg2.extras
    POSTGRESQL_AVAILABLE = True
except ImportError:
    POSTGRESQL_AVAILABLE = False

try:
    import mysql.connector
    MYSQL_AVAILABLE = True
except ImportError:
    MYSQL_AVAILABLE = False

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

@dataclass
class DatabaseConfig:
    """æ•°æ®åº“é…ç½®"""
    name: str
    type: str  # sqlite, postgresql, mysql, mongodb, redis
    host: str = 'localhost'
    port: int = None
    username: str = None
    password: str = None
    database: str = None
    options: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.options is None:
            self.options = {}
        
        # è®¾ç½®é»˜è®¤ç«¯å£
        if self.port is None:
            default_ports = {
                'postgresql': 5432,
                'mysql': 3306,
                'mongodb': 27017,
                'redis': 6379
            }
            self.port = default_ports.get(self.type)

@dataclass
class QueryResult:
    """æŸ¥è¯¢ç»“æœ"""
    success: bool
    data: Any = None
    affected_rows: int = 0
    execution_time: float = 0
    error: str = None

@dataclass
class BackupInfo:
    """å¤‡ä»½ä¿¡æ¯"""
    name: str
    database: str
    timestamp: str
    size: int
    path: str
    type: str  # full, incremental, schema

class DatabaseManager:
    """æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, project_root: str = None):
        """åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨"""
        if project_root is None:
            self.project_root = Path(__file__).parent.parent.parent
        else:
            self.project_root = Path(project_root)
        
        self.config_dir = self.project_root / "Struc" / "GeneralOffice" / "config"
        self.data_dir = self.project_root / "data"
        self.backup_dir = self.project_root / "backups" / "database"
        self.logs_dir = self.project_root / "logs" / "database"
        
        # åˆ›å»ºç›®å½•
        for directory in [self.data_dir, self.backup_dir, self.logs_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½é…ç½®
        self.config = self._load_config()
        self.databases = self._load_database_configs()
        
        # è¿æ¥æ± 
        self.connections = {}
        self.connection_lock = threading.Lock()
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®åº“ç®¡ç†é…ç½®"""
        config_file = self.config_dir / "database_config.yaml"
        
        default_config = {
            'connection_pool': {
                'max_connections': 10,
                'timeout': 30,
                'retry_attempts': 3,
                'retry_delay': 1
            },
            'backup': {
                'auto_backup': True,
                'backup_schedule': 'daily',  # daily, weekly, monthly
                'retention_days': 30,
                'compression': True,
                'backup_types': ['full', 'schema']
            },
            'monitoring': {
                'enable_query_logging': True,
                'slow_query_threshold': 1.0,  # ç§’
                'enable_performance_monitoring': True
            },
            'security': {
                'encrypt_passwords': True,
                'ssl_required': False,
                'connection_timeout': 30
            },
            'maintenance': {
                'auto_vacuum': True,
                'analyze_tables': True,
                'cleanup_logs': True,
                'log_retention_days': 7
            },
            'log_level': 'INFO'
        }
        
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    default_config.update(user_config)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ•°æ®åº“é…ç½®å¤±è´¥: {e}")
        
        return default_config
    
    def _load_database_configs(self) -> Dict[str, DatabaseConfig]:
        """åŠ è½½æ•°æ®åº“é…ç½®"""
        databases = {}
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½
        db_config_file = self.config_dir / "databases.yaml"
        
        if db_config_file.exists():
            try:
                with open(db_config_file, 'r', encoding='utf-8') as f:
                    db_configs = yaml.safe_load(f)
                    
                    for name, config in db_configs.items():
                        databases[name] = DatabaseConfig(
                            name=name,
                            **config
                        )
            except Exception as e:
                self.logger.error(f"åŠ è½½æ•°æ®åº“é…ç½®å¤±è´¥: {e}")
        
        # é»˜è®¤SQLiteæ•°æ®åº“
        if 'default' not in databases:
            sqlite_path = self.data_dir / "default.db"
            databases['default'] = DatabaseConfig(
                name='default',
                type='sqlite',
                database=str(sqlite_path)
            )
        
        return databases
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.logs_dir / f"database_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=getattr(logging, self.config['log_level']),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def add_database(self, config: DatabaseConfig) -> bool:
        """æ·»åŠ æ•°æ®åº“é…ç½®"""
        try:
            self.databases[config.name] = config
            self._save_database_configs()
            
            self.logger.info(f"æ•°æ®åº“é…ç½®å·²æ·»åŠ : {config.name}")
            return True
        
        except Exception as e:
            self.logger.error(f"æ·»åŠ æ•°æ®åº“é…ç½®å¤±è´¥: {e}")
            return False
    
    def remove_database(self, name: str) -> bool:
        """ç§»é™¤æ•°æ®åº“é…ç½®"""
        try:
            if name in self.databases:
                # å…³é—­è¿æ¥
                self.disconnect(name)
                
                # ç§»é™¤é…ç½®
                del self.databases[name]
                self._save_database_configs()
                
                self.logger.info(f"æ•°æ®åº“é…ç½®å·²ç§»é™¤: {name}")
                return True
            else:
                self.logger.warning(f"æ•°æ®åº“é…ç½®ä¸å­˜åœ¨: {name}")
                return False
        
        except Exception as e:
            self.logger.error(f"ç§»é™¤æ•°æ®åº“é…ç½®å¤±è´¥: {e}")
            return False
    
    def _save_database_configs(self):
        """ä¿å­˜æ•°æ®åº“é…ç½®"""
        db_config_file = self.config_dir / "databases.yaml"
        
        try:
            configs = {}
            for name, config in self.databases.items():
                config_dict = asdict(config)
                del config_dict['name']  # åç§°ä½œä¸ºkey
                configs[name] = config_dict
            
            with open(db_config_file, 'w', encoding='utf-8') as f:
                yaml.dump(configs, f, default_flow_style=False, allow_unicode=True)
        
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®åº“é…ç½®å¤±è´¥: {e}")
    
    @contextmanager
    def get_connection(self, database_name: str = 'default'):
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
        connection = None
        
        try:
            connection = self.connect(database_name)
            yield connection
        
        finally:
            if connection:
                self.disconnect(database_name)
    
    def connect(self, database_name: str = 'default') -> Any:
        """è¿æ¥æ•°æ®åº“"""
        if database_name not in self.databases:
            raise ValueError(f"æ•°æ®åº“é…ç½®ä¸å­˜åœ¨: {database_name}")
        
        with self.connection_lock:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¿æ¥
            if database_name in self.connections:
                return self.connections[database_name]
            
            config = self.databases[database_name]
            connection = None
            
            try:
                if config.type == 'sqlite':
                    connection = self._connect_sqlite(config)
                
                elif config.type == 'postgresql':
                    connection = self._connect_postgresql(config)
                
                elif config.type == 'mysql':
                    connection = self._connect_mysql(config)
                
                elif config.type == 'mongodb':
                    connection = self._connect_mongodb(config)
                
                elif config.type == 'redis':
                    connection = self._connect_redis(config)
                
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {config.type}")
                
                self.connections[database_name] = connection
                self.logger.info(f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {database_name}")
                
                return connection
            
            except Exception as e:
                self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥ {database_name}: {e}")
                raise
    
    def _connect_sqlite(self, config: DatabaseConfig) -> sqlite3.Connection:
        """è¿æ¥SQLiteæ•°æ®åº“"""
        # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
        db_path = Path(config.database)
        db_path.parent.mkdir(parents=True, exist_ok=True)
        
        connection = sqlite3.connect(
            config.database,
            timeout=self.config['connection_pool']['timeout'],
            **config.options
        )
        
        # è®¾ç½®è¡Œå·¥å‚
        connection.row_factory = sqlite3.Row
        
        # å¯ç”¨å¤–é”®çº¦æŸ
        connection.execute("PRAGMA foreign_keys = ON")
        
        return connection
    
    def _connect_postgresql(self, config: DatabaseConfig) -> Any:
        """è¿æ¥PostgreSQLæ•°æ®åº“"""
        if not POSTGRESQL_AVAILABLE:
            raise ImportError("PostgreSQLæ”¯æŒéœ€è¦å®‰è£…psycopg2: pip install psycopg2-binary")
        
        connection = psycopg2.connect(
            host=config.host,
            port=config.port,
            user=config.username,
            password=config.password,
            database=config.database,
            connect_timeout=self.config['security']['connection_timeout'],
            **config.options
        )
        
        # è®¾ç½®è‡ªåŠ¨æäº¤
        connection.autocommit = True
        
        return connection
    
    def _connect_mysql(self, config: DatabaseConfig) -> Any:
        """è¿æ¥MySQLæ•°æ®åº“"""
        if not MYSQL_AVAILABLE:
            raise ImportError("MySQLæ”¯æŒéœ€è¦å®‰è£…mysql-connector-python: pip install mysql-connector-python")
        
        connection = mysql.connector.connect(
            host=config.host,
            port=config.port,
            user=config.username,
            password=config.password,
            database=config.database,
            connection_timeout=self.config['security']['connection_timeout'],
            **config.options
        )
        
        return connection
    
    def _connect_mongodb(self, config: DatabaseConfig) -> Any:
        """è¿æ¥MongoDBæ•°æ®åº“"""
        if not MONGODB_AVAILABLE:
            raise ImportError("MongoDBæ”¯æŒéœ€è¦å®‰è£…pymongo: pip install pymongo")
        
        # æ„å»ºè¿æ¥å­—ç¬¦ä¸²
        if config.username and config.password:
            connection_string = f"mongodb://{config.username}:{config.password}@{config.host}:{config.port}/{config.database}"
        else:
            connection_string = f"mongodb://{config.host}:{config.port}/{config.database}"
        
        client = pymongo.MongoClient(
            connection_string,
            serverSelectionTimeoutMS=self.config['security']['connection_timeout'] * 1000,
            **config.options
        )
        
        return client[config.database]
    
    def _connect_redis(self, config: DatabaseConfig) -> Any:
        """è¿æ¥Redisæ•°æ®åº“"""
        if not REDIS_AVAILABLE:
            raise ImportError("Redisæ”¯æŒéœ€è¦å®‰è£…redis: pip install redis")
        
        connection = redis.Redis(
            host=config.host,
            port=config.port,
            password=config.password,
            db=config.database or 0,
            socket_timeout=self.config['security']['connection_timeout'],
            **config.options
        )
        
        # æµ‹è¯•è¿æ¥
        connection.ping()
        
        return connection
    
    def disconnect(self, database_name: str = None):
        """æ–­å¼€æ•°æ®åº“è¿æ¥"""
        with self.connection_lock:
            if database_name:
                if database_name in self.connections:
                    try:
                        connection = self.connections[database_name]
                        
                        # æ ¹æ®æ•°æ®åº“ç±»å‹å…³é—­è¿æ¥
                        config = self.databases[database_name]
                        
                        if config.type in ['sqlite', 'postgresql', 'mysql']:
                            connection.close()
                        elif config.type == 'mongodb':
                            connection.client.close()
                        elif config.type == 'redis':
                            connection.close()
                        
                        del self.connections[database_name]
                        self.logger.info(f"æ•°æ®åº“è¿æ¥å·²å…³é—­: {database_name}")
                    
                    except Exception as e:
                        self.logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥ {database_name}: {e}")
            else:
                # å…³é—­æ‰€æœ‰è¿æ¥
                for name in list(self.connections.keys()):
                    self.disconnect(name)
    
    def execute_query(self, query: str, params: Tuple = None, database_name: str = 'default') -> QueryResult:
        """æ‰§è¡ŒSQLæŸ¥è¯¢"""
        start_time = time.time()
        
        try:
            with self.get_connection(database_name) as connection:
                config = self.databases[database_name]
                
                if config.type == 'sqlite':
                    return self._execute_sqlite_query(connection, query, params)
                
                elif config.type == 'postgresql':
                    return self._execute_postgresql_query(connection, query, params)
                
                elif config.type == 'mysql':
                    return self._execute_mysql_query(connection, query, params)
                
                else:
                    raise ValueError(f"SQLæŸ¥è¯¢ä¸æ”¯æŒæ•°æ®åº“ç±»å‹: {config.type}")
        
        except Exception as e:
            execution_time = time.time() - start_time
            self.logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            
            return QueryResult(
                success=False,
                execution_time=execution_time,
                error=str(e)
            )
    
    def _execute_sqlite_query(self, connection: sqlite3.Connection, query: str, params: Tuple) -> QueryResult:
        """æ‰§è¡ŒSQLiteæŸ¥è¯¢"""
        start_time = time.time()
        
        try:
            cursor = connection.cursor()
            
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            # åˆ¤æ–­æŸ¥è¯¢ç±»å‹
            query_type = query.strip().upper().split()[0]
            
            if query_type == 'SELECT':
                data = [dict(row) for row in cursor.fetchall()]
                affected_rows = len(data)
            else:
                data = None
                affected_rows = cursor.rowcount
                connection.commit()
            
            execution_time = time.time() - start_time
            
            # è®°å½•æ…¢æŸ¥è¯¢
            if execution_time > self.config['monitoring']['slow_query_threshold']:
                self.logger.warning(f"æ…¢æŸ¥è¯¢æ£€æµ‹ ({execution_time:.2f}s): {query[:100]}...")
            
            return QueryResult(
                success=True,
                data=data,
                affected_rows=affected_rows,
                execution_time=execution_time
            )
        
        except Exception as e:
            execution_time = time.time() - start_time
            raise e
    
    def _execute_postgresql_query(self, connection: Any, query: str, params: Tuple) -> QueryResult:
        """æ‰§è¡ŒPostgreSQLæŸ¥è¯¢"""
        start_time = time.time()
        
        try:
            cursor = connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            # åˆ¤æ–­æŸ¥è¯¢ç±»å‹
            query_type = query.strip().upper().split()[0]
            
            if query_type == 'SELECT':
                data = [dict(row) for row in cursor.fetchall()]
                affected_rows = len(data)
            else:
                data = None
                affected_rows = cursor.rowcount
            
            execution_time = time.time() - start_time
            
            return QueryResult(
                success=True,
                data=data,
                affected_rows=affected_rows,
                execution_time=execution_time
            )
        
        except Exception as e:
            execution_time = time.time() - start_time
            raise e
    
    def _execute_mysql_query(self, connection: Any, query: str, params: Tuple) -> QueryResult:
        """æ‰§è¡ŒMySQLæŸ¥è¯¢"""
        start_time = time.time()
        
        try:
            cursor = connection.cursor(dictionary=True)
            
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            # åˆ¤æ–­æŸ¥è¯¢ç±»å‹
            query_type = query.strip().upper().split()[0]
            
            if query_type == 'SELECT':
                data = cursor.fetchall()
                affected_rows = len(data)
            else:
                data = None
                affected_rows = cursor.rowcount
                connection.commit()
            
            execution_time = time.time() - start_time
            
            return QueryResult(
                success=True,
                data=data,
                affected_rows=affected_rows,
                execution_time=execution_time
            )
        
        except Exception as e:
            execution_time = time.time() - start_time
            raise e
    
    def create_table(self, table_name: str, columns: Dict[str, str], database_name: str = 'default') -> bool:
        """åˆ›å»ºè¡¨"""
        try:
            config = self.databases[database_name]
            
            if config.type == 'sqlite':
                column_defs = []
                for col_name, col_type in columns.items():
                    column_defs.append(f"{col_name} {col_type}")
                
                query = f"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(column_defs)})"
            
            elif config.type in ['postgresql', 'mysql']:
                column_defs = []
                for col_name, col_type in columns.items():
                    column_defs.append(f"{col_name} {col_type}")
                
                query = f"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(column_defs)})"
            
            else:
                raise ValueError(f"åˆ›å»ºè¡¨ä¸æ”¯æŒæ•°æ®åº“ç±»å‹: {config.type}")
            
            result = self.execute_query(query, database_name=database_name)
            
            if result.success:
                self.logger.info(f"è¡¨åˆ›å»ºæˆåŠŸ: {table_name}")
                return True
            else:
                self.logger.error(f"è¡¨åˆ›å»ºå¤±è´¥: {result.error}")
                return False
        
        except Exception as e:
            self.logger.error(f"åˆ›å»ºè¡¨å¤±è´¥: {e}")
            return False
    
    def insert_data(self, table_name: str, data: Union[Dict, List[Dict]], database_name: str = 'default') -> bool:
        """æ’å…¥æ•°æ®"""
        try:
            if isinstance(data, dict):
                data = [data]
            
            if not data:
                return True
            
            # è·å–åˆ—å
            columns = list(data[0].keys())
            placeholders = ', '.join(['?' if self.databases[database_name].type == 'sqlite' else '%s'] * len(columns))
            
            query = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})"
            
            # æ‰¹é‡æ’å…¥
            success_count = 0
            
            for row in data:
                values = tuple(row[col] for col in columns)
                result = self.execute_query(query, values, database_name)
                
                if result.success:
                    success_count += 1
                else:
                    self.logger.error(f"æ’å…¥æ•°æ®å¤±è´¥: {result.error}")
            
            self.logger.info(f"æ•°æ®æ’å…¥å®Œæˆ: {success_count}/{len(data)} æ¡è®°å½•")
            return success_count == len(data)
        
        except Exception as e:
            self.logger.error(f"æ’å…¥æ•°æ®å¤±è´¥: {e}")
            return False
    
    def update_data(self, table_name: str, data: Dict, where_clause: str, params: Tuple = None, database_name: str = 'default') -> bool:
        """æ›´æ–°æ•°æ®"""
        try:
            set_clauses = []
            values = []
            
            for column, value in data.items():
                if self.databases[database_name].type == 'sqlite':
                    set_clauses.append(f"{column} = ?")
                else:
                    set_clauses.append(f"{column} = %s")
                values.append(value)
            
            query = f"UPDATE {table_name} SET {', '.join(set_clauses)} WHERE {where_clause}"
            
            if params:
                values.extend(params)
            
            result = self.execute_query(query, tuple(values), database_name)
            
            if result.success:
                self.logger.info(f"æ•°æ®æ›´æ–°æˆåŠŸ: {result.affected_rows} æ¡è®°å½•")
                return True
            else:
                self.logger.error(f"æ•°æ®æ›´æ–°å¤±è´¥: {result.error}")
                return False
        
        except Exception as e:
            self.logger.error(f"æ›´æ–°æ•°æ®å¤±è´¥: {e}")
            return False
    
    def delete_data(self, table_name: str, where_clause: str, params: Tuple = None, database_name: str = 'default') -> bool:
        """åˆ é™¤æ•°æ®"""
        try:
            query = f"DELETE FROM {table_name} WHERE {where_clause}"
            
            result = self.execute_query(query, params, database_name)
            
            if result.success:
                self.logger.info(f"æ•°æ®åˆ é™¤æˆåŠŸ: {result.affected_rows} æ¡è®°å½•")
                return True
            else:
                self.logger.error(f"æ•°æ®åˆ é™¤å¤±è´¥: {result.error}")
                return False
        
        except Exception as e:
            self.logger.error(f"åˆ é™¤æ•°æ®å¤±è´¥: {e}")
            return False
    
    def select_data(self, table_name: str, columns: str = '*', where_clause: str = None, params: Tuple = None, database_name: str = 'default') -> List[Dict]:
        """æŸ¥è¯¢æ•°æ®"""
        try:
            query = f"SELECT {columns} FROM {table_name}"
            
            if where_clause:
                query += f" WHERE {where_clause}"
            
            result = self.execute_query(query, params, database_name)
            
            if result.success:
                return result.data or []
            else:
                self.logger.error(f"æ•°æ®æŸ¥è¯¢å¤±è´¥: {result.error}")
                return []
        
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢æ•°æ®å¤±è´¥: {e}")
            return []
    
    def backup_database(self, database_name: str = 'default', backup_type: str = 'full') -> Optional[BackupInfo]:
        """å¤‡ä»½æ•°æ®åº“"""
        try:
            config = self.databases[database_name]
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            backup_name = f"{database_name}_{backup_type}_{timestamp}"
            
            if config.type == 'sqlite':
                return self._backup_sqlite(config, backup_name, backup_type)
            
            elif config.type == 'postgresql':
                return self._backup_postgresql(config, backup_name, backup_type)
            
            elif config.type == 'mysql':
                return self._backup_mysql(config, backup_name, backup_type)
            
            else:
                self.logger.error(f"å¤‡ä»½ä¸æ”¯æŒæ•°æ®åº“ç±»å‹: {config.type}")
                return None
        
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
            return None
    
    def _backup_sqlite(self, config: DatabaseConfig, backup_name: str, backup_type: str) -> BackupInfo:
        """å¤‡ä»½SQLiteæ•°æ®åº“"""
        source_path = Path(config.database)
        backup_path = self.backup_dir / f"{backup_name}.db"
        
        if backup_type == 'full':
            # å®Œæ•´å¤‡ä»½
            shutil.copy2(source_path, backup_path)
        
        elif backup_type == 'schema':
            # ä»…å¤‡ä»½ç»“æ„
            with sqlite3.connect(source_path) as source_conn:
                with sqlite3.connect(backup_path) as backup_conn:
                    # è·å–æ‰€æœ‰è¡¨çš„åˆ›å»ºè¯­å¥
                    cursor = source_conn.cursor()
                    cursor.execute("SELECT sql FROM sqlite_master WHERE type='table'")
                    
                    for (sql,) in cursor.fetchall():
                        if sql:
                            backup_conn.execute(sql)
        
        # å‹ç¼©å¤‡ä»½æ–‡ä»¶
        if self.config['backup']['compression']:
            compressed_path = backup_path.with_suffix('.db.gz')
            
            import gzip
            with open(backup_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            backup_path.unlink()  # åˆ é™¤æœªå‹ç¼©æ–‡ä»¶
            backup_path = compressed_path
        
        backup_info = BackupInfo(
            name=backup_name,
            database=config.name,
            timestamp=datetime.now().isoformat(),
            size=backup_path.stat().st_size,
            path=str(backup_path),
            type=backup_type
        )
        
        self.logger.info(f"SQLiteå¤‡ä»½å®Œæˆ: {backup_path}")
        return backup_info
    
    def _backup_postgresql(self, config: DatabaseConfig, backup_name: str, backup_type: str) -> BackupInfo:
        """å¤‡ä»½PostgreSQLæ•°æ®åº“"""
        backup_path = self.backup_dir / f"{backup_name}.sql"
        
        # æ„å»ºpg_dumpå‘½ä»¤
        cmd = [
            'pg_dump',
            '-h', config.host,
            '-p', str(config.port),
            '-U', config.username,
            '-d', config.database,
            '-f', str(backup_path)
        ]
        
        if backup_type == 'schema':
            cmd.append('--schema-only')
        
        # è®¾ç½®å¯†ç ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['PGPASSWORD'] = config.password
        
        # æ‰§è¡Œå¤‡ä»½
        result = subprocess.run(cmd, env=env, capture_output=True, text=True)
        
        if result.returncode != 0:
            raise Exception(f"pg_dumpå¤±è´¥: {result.stderr}")
        
        # å‹ç¼©å¤‡ä»½æ–‡ä»¶
        if self.config['backup']['compression']:
            compressed_path = backup_path.with_suffix('.sql.gz')
            
            import gzip
            with open(backup_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            backup_path.unlink()
            backup_path = compressed_path
        
        backup_info = BackupInfo(
            name=backup_name,
            database=config.name,
            timestamp=datetime.now().isoformat(),
            size=backup_path.stat().st_size,
            path=str(backup_path),
            type=backup_type
        )
        
        self.logger.info(f"PostgreSQLå¤‡ä»½å®Œæˆ: {backup_path}")
        return backup_info
    
    def _backup_mysql(self, config: DatabaseConfig, backup_name: str, backup_type: str) -> BackupInfo:
        """å¤‡ä»½MySQLæ•°æ®åº“"""
        backup_path = self.backup_dir / f"{backup_name}.sql"
        
        # æ„å»ºmysqldumpå‘½ä»¤
        cmd = [
            'mysqldump',
            '-h', config.host,
            '-P', str(config.port),
            '-u', config.username,
            f'-p{config.password}',
            config.database
        ]
        
        if backup_type == 'schema':
            cmd.append('--no-data')
        
        # æ‰§è¡Œå¤‡ä»½
        with open(backup_path, 'w') as f:
            result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True)
        
        if result.returncode != 0:
            raise Exception(f"mysqldumpå¤±è´¥: {result.stderr}")
        
        # å‹ç¼©å¤‡ä»½æ–‡ä»¶
        if self.config['backup']['compression']:
            compressed_path = backup_path.with_suffix('.sql.gz')
            
            import gzip
            with open(backup_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            backup_path.unlink()
            backup_path = compressed_path
        
        backup_info = BackupInfo(
            name=backup_name,
            database=config.name,
            timestamp=datetime.now().isoformat(),
            size=backup_path.stat().st_size,
            path=str(backup_path),
            type=backup_type
        )
        
        self.logger.info(f"MySQLå¤‡ä»½å®Œæˆ: {backup_path}")
        return backup_info
    
    def restore_database(self, backup_path: str, database_name: str = 'default') -> bool:
        """æ¢å¤æ•°æ®åº“"""
        try:
            config = self.databases[database_name]
            backup_file = Path(backup_path)
            
            if not backup_file.exists():
                self.logger.error(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")
                return False
            
            if config.type == 'sqlite':
                return self._restore_sqlite(config, backup_file)
            
            elif config.type == 'postgresql':
                return self._restore_postgresql(config, backup_file)
            
            elif config.type == 'mysql':
                return self._restore_mysql(config, backup_file)
            
            else:
                self.logger.error(f"æ¢å¤ä¸æ”¯æŒæ•°æ®åº“ç±»å‹: {config.type}")
                return False
        
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“æ¢å¤å¤±è´¥: {e}")
            return False
    
    def _restore_sqlite(self, config: DatabaseConfig, backup_file: Path) -> bool:
        """æ¢å¤SQLiteæ•°æ®åº“"""
        try:
            # å…³é—­ç°æœ‰è¿æ¥
            self.disconnect(config.name)
            
            # å¤‡ä»½å½“å‰æ•°æ®åº“
            current_db = Path(config.database)
            if current_db.exists():
                backup_current = current_db.with_suffix(f'.backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.db')
                shutil.copy2(current_db, backup_current)
                self.logger.info(f"å½“å‰æ•°æ®åº“å·²å¤‡ä»½è‡³: {backup_current}")
            
            # è§£å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if backup_file.suffix == '.gz':
                import gzip
                temp_file = backup_file.with_suffix('')
                
                with gzip.open(backup_file, 'rb') as f_in:
                    with open(temp_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                
                backup_file = temp_file
            
            # æ¢å¤æ•°æ®åº“
            shutil.copy2(backup_file, current_db)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if backup_file.name.endswith('.backup'):
                backup_file.unlink()
            
            self.logger.info(f"SQLiteæ•°æ®åº“æ¢å¤å®Œæˆ: {config.database}")
            return True
        
        except Exception as e:
            self.logger.error(f"SQLiteæ¢å¤å¤±è´¥: {e}")
            return False
    
    def _restore_postgresql(self, config: DatabaseConfig, backup_file: Path) -> bool:
        """æ¢å¤PostgreSQLæ•°æ®åº“"""
        try:
            # è§£å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
            restore_file = backup_file
            
            if backup_file.suffix == '.gz':
                import gzip
                temp_file = backup_file.with_suffix('')
                
                with gzip.open(backup_file, 'rb') as f_in:
                    with open(temp_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                
                restore_file = temp_file
            
            # æ„å»ºpsqlå‘½ä»¤
            cmd = [
                'psql',
                '-h', config.host,
                '-p', str(config.port),
                '-U', config.username,
                '-d', config.database,
                '-f', str(restore_file)
            ]
            
            # è®¾ç½®å¯†ç ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['PGPASSWORD'] = config.password
            
            # æ‰§è¡Œæ¢å¤
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if restore_file != backup_file:
                restore_file.unlink()
            
            if result.returncode != 0:
                raise Exception(f"psqlå¤±è´¥: {result.stderr}")
            
            self.logger.info(f"PostgreSQLæ•°æ®åº“æ¢å¤å®Œæˆ")
            return True
        
        except Exception as e:
            self.logger.error(f"PostgreSQLæ¢å¤å¤±è´¥: {e}")
            return False
    
    def _restore_mysql(self, config: DatabaseConfig, backup_file: Path) -> bool:
        """æ¢å¤MySQLæ•°æ®åº“"""
        try:
            # è§£å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
            restore_file = backup_file
            
            if backup_file.suffix == '.gz':
                import gzip
                temp_file = backup_file.with_suffix('')
                
                with gzip.open(backup_file, 'rb') as f_in:
                    with open(temp_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                
                restore_file = temp_file
            
            # æ„å»ºmysqlå‘½ä»¤
            cmd = [
                'mysql',
                '-h', config.host,
                '-P', str(config.port),
                '-u', config.username,
                f'-p{config.password}',
                config.database
            ]
            
            # æ‰§è¡Œæ¢å¤
            with open(restore_file, 'r') as f:
                result = subprocess.run(cmd, stdin=f, capture_output=True, text=True)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if restore_file != backup_file:
                restore_file.unlink()
            
            if result.returncode != 0:
                raise Exception(f"mysqlå¤±è´¥: {result.stderr}")
            
            self.logger.info(f"MySQLæ•°æ®åº“æ¢å¤å®Œæˆ")
            return True
        
        except Exception as e:
            self.logger.error(f"MySQLæ¢å¤å¤±è´¥: {e}")
            return False
    
    def list_backups(self, database_name: str = None) -> List[BackupInfo]:
        """åˆ—å‡ºå¤‡ä»½æ–‡ä»¶"""
        backups = []
        
        try:
            for backup_file in self.backup_dir.glob('*'):
                if backup_file.is_file():
                    # è§£æå¤‡ä»½æ–‡ä»¶å
                    name_parts = backup_file.stem.split('_')
                    
                    if len(name_parts) >= 3:
                        db_name = name_parts[0]
                        backup_type = name_parts[1]
                        
                        # è¿‡æ»¤æŒ‡å®šæ•°æ®åº“
                        if database_name and db_name != database_name:
                            continue
                        
                        backup_info = BackupInfo(
                            name=backup_file.stem,
                            database=db_name,
                            timestamp=datetime.fromtimestamp(backup_file.stat().st_mtime).isoformat(),
                            size=backup_file.stat().st_size,
                            path=str(backup_file),
                            type=backup_type
                        )
                        
                        backups.append(backup_info)
            
            # æŒ‰æ—¶é—´æ’åº
            backups.sort(key=lambda x: x.timestamp, reverse=True)
        
        except Exception as e:
            self.logger.error(f"åˆ—å‡ºå¤‡ä»½å¤±è´¥: {e}")
        
        return backups
    
    def cleanup_old_backups(self, retention_days: int = None) -> int:
        """æ¸…ç†æ—§å¤‡ä»½"""
        if retention_days is None:
            retention_days = self.config['backup']['retention_days']
        
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        deleted_count = 0
        
        try:
            for backup_file in self.backup_dir.glob('*'):
                if backup_file.is_file():
                    file_time = datetime.fromtimestamp(backup_file.stat().st_mtime)
                    
                    if file_time < cutoff_date:
                        backup_file.unlink()
                        deleted_count += 1
                        self.logger.info(f"åˆ é™¤æ—§å¤‡ä»½: {backup_file.name}")
        
        except Exception as e:
            self.logger.error(f"æ¸…ç†å¤‡ä»½å¤±è´¥: {e}")
        
        return deleted_count
    
    def get_database_info(self, database_name: str = 'default') -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        try:
            config = self.databases[database_name]
            info = {
                'name': config.name,
                'type': config.type,
                'host': config.host,
                'port': config.port,
                'database': config.database,
                'connected': database_name in self.connections
            }
            
            if config.type == 'sqlite':
                db_path = Path(config.database)
                if db_path.exists():
                    info['size'] = db_path.stat().st_size
                    info['modified'] = datetime.fromtimestamp(db_path.stat().st_mtime).isoformat()
                
                # è·å–è¡¨ä¿¡æ¯
                try:
                    tables = self.select_data('sqlite_master', 'name', "type='table'", database_name=database_name)
                    info['tables'] = [table['name'] for table in tables]
                except:
                    info['tables'] = []
            
            return info
        
        except Exception as e:
            self.logger.error(f"è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def list_databases(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ•°æ®åº“"""
        databases = []
        
        for name, config in self.databases.items():
            db_info = self.get_database_info(name)
            databases.append(db_info)
        
        return databases

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='YDS-Lab æ•°æ®åº“ç®¡ç†å·¥å…·')
    parser.add_argument('--project-root', help='æŒ‡å®šé¡¹ç›®æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæ‰€æœ‰æ•°æ®åº“')
    parser.add_argument('--info', help='æ˜¾ç¤ºæŒ‡å®šæ•°æ®åº“ä¿¡æ¯')
    parser.add_argument('--backup', help='å¤‡ä»½æŒ‡å®šæ•°æ®åº“')
    parser.add_argument('--backup-type', choices=['full', 'schema'], default='full', help='å¤‡ä»½ç±»å‹')
    parser.add_argument('--restore', nargs=2, metavar=('BACKUP_PATH', 'DATABASE'), help='æ¢å¤æ•°æ®åº“')
    parser.add_argument('--list-backups', help='åˆ—å‡ºæŒ‡å®šæ•°æ®åº“çš„å¤‡ä»½')
    parser.add_argument('--cleanup', action='store_true', help='æ¸…ç†æ—§å¤‡ä»½')
    parser.add_argument('--query', nargs=2, metavar=('SQL', 'DATABASE'), help='æ‰§è¡ŒSQLæŸ¥è¯¢')
    parser.add_argument('--create-table', nargs=3, metavar=('TABLE', 'COLUMNS', 'DATABASE'), help='åˆ›å»ºè¡¨')
    
    args = parser.parse_args()
    
    manager = DatabaseManager(args.project_root)
    
    # åˆ—å‡ºæ•°æ®åº“
    if args.list:
        databases = manager.list_databases()
        
        print("ğŸ“Š æ•°æ®åº“åˆ—è¡¨")
        print("="*50)
        
        for db in databases:
            print(f"åç§°: {db['name']}")
            print(f"ç±»å‹: {db['type']}")
            print(f"è¿æ¥çŠ¶æ€: {'å·²è¿æ¥' if db['connected'] else 'æœªè¿æ¥'}")
            
            if 'size' in db:
                print(f"å¤§å°: {db['size'] / (1024*1024):.1f} MB")
            
            if 'tables' in db:
                print(f"è¡¨æ•°é‡: {len(db['tables'])}")
            
            print("-" * 30)
        
        return
    
    # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
    if args.info:
        info = manager.get_database_info(args.info)
        
        if info:
            print(f"ğŸ“‹ æ•°æ®åº“ä¿¡æ¯: {args.info}")
            print("="*40)
            
            for key, value in info.items():
                print(f"{key}: {value}")
        else:
            print(f"âŒ æ•°æ®åº“ä¸å­˜åœ¨: {args.info}")
        
        return
    
    # å¤‡ä»½æ•°æ®åº“
    if args.backup:
        print(f"ğŸ”„ å¤‡ä»½æ•°æ®åº“: {args.backup}")
        
        backup_info = manager.backup_database(args.backup, args.backup_type)
        
        if backup_info:
            print(f"âœ… å¤‡ä»½å®Œæˆ:")
            print(f"  æ–‡ä»¶: {backup_info.path}")
            print(f"  å¤§å°: {backup_info.size / (1024*1024):.1f} MB")
            print(f"  ç±»å‹: {backup_info.type}")
        else:
            print("âŒ å¤‡ä»½å¤±è´¥")
        
        return
    
    # æ¢å¤æ•°æ®åº“
    if args.restore:
        backup_path, database_name = args.restore
        
        print(f"ğŸ”„ æ¢å¤æ•°æ®åº“: {database_name}")
        
        if manager.restore_database(backup_path, database_name):
            print("âœ… æ¢å¤å®Œæˆ")
        else:
            print("âŒ æ¢å¤å¤±è´¥")
        
        return
    
    # åˆ—å‡ºå¤‡ä»½
    if args.list_backups:
        backups = manager.list_backups(args.list_backups)
        
        print(f"ğŸ“¦ å¤‡ä»½åˆ—è¡¨: {args.list_backups}")
        print("="*50)
        
        for backup in backups:
            print(f"åç§°: {backup.name}")
            print(f"æ—¶é—´: {backup.timestamp}")
            print(f"å¤§å°: {backup.size / (1024*1024):.1f} MB")
            print(f"ç±»å‹: {backup.type}")
            print(f"è·¯å¾„: {backup.path}")
            print("-" * 30)
        
        return
    
    # æ¸…ç†å¤‡ä»½
    if args.cleanup:
        deleted_count = manager.cleanup_old_backups()
        print(f"ğŸ§¹ æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§å¤‡ä»½")
        return
    
    # æ‰§è¡ŒæŸ¥è¯¢
    if args.query:
        sql, database_name = args.query
        
        print(f"ğŸ” æ‰§è¡ŒæŸ¥è¯¢: {database_name}")
        
        result = manager.execute_query(sql, database_name=database_name)
        
        if result.success:
            print(f"âœ… æŸ¥è¯¢æˆåŠŸ (è€—æ—¶: {result.execution_time:.3f}s)")
            
            if result.data:
                print(f"è¿”å› {len(result.data)} æ¡è®°å½•:")
                
                for i, row in enumerate(result.data[:10]):  # æ˜¾ç¤ºå‰10æ¡
                    print(f"  {i+1}: {row}")
                
                if len(result.data) > 10:
                    print(f"  ... è¿˜æœ‰ {len(result.data) - 10} æ¡è®°å½•")
            
            elif result.affected_rows > 0:
                print(f"å½±å“ {result.affected_rows} æ¡è®°å½•")
        else:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {result.error}")
        
        return
    
    # åˆ›å»ºè¡¨
    if args.create_table:
        table_name, columns_str, database_name = args.create_table
        
        try:
            # è§£æåˆ—å®šä¹‰ (æ ¼å¼: "name:TEXT,age:INTEGER")
            columns = {}
            for col_def in columns_str.split(','):
                col_name, col_type = col_def.split(':')
                columns[col_name.strip()] = col_type.strip()
            
            print(f"ğŸ”¨ åˆ›å»ºè¡¨: {table_name}")
            
            if manager.create_table(table_name, columns, database_name):
                print("âœ… è¡¨åˆ›å»ºæˆåŠŸ")
            else:
                print("âŒ è¡¨åˆ›å»ºå¤±è´¥")
        
        except Exception as e:
            print(f"âŒ å‚æ•°è§£æå¤±è´¥: {e}")
        
        return
    
    # é»˜è®¤æ˜¾ç¤ºçŠ¶æ€
    print("ğŸ“Š æ•°æ®åº“ç®¡ç†å™¨çŠ¶æ€")
    print("="*40)
    print(f"é¡¹ç›®è·¯å¾„: {manager.project_root}")
    print(f"æ•°æ®åº“æ•°é‡: {len(manager.databases)}")
    print(f"æ´»åŠ¨è¿æ¥: {len(manager.connections)}")
    
    # æ˜¾ç¤ºé…ç½®çš„æ•°æ®åº“
    for name, config in manager.databases.items():
        status = "å·²è¿æ¥" if name in manager.connections else "æœªè¿æ¥"
        print(f"  {name} ({config.type}): {status}")

if __name__ == "__main__":
    main()