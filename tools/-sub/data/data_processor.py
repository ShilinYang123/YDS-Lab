#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YDS-Lab æ•°æ®å¤„ç†å·¥å…·
æä¾›æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€åˆ†æã€å¯è§†åŒ–å’Œå¯¼å…¥å¯¼å‡ºåŠŸèƒ½
é€‚é…YDS-Labé¡¹ç›®ç»“æ„å’ŒAI Agentåä½œéœ€æ±‚
"""

import os
import sys
import json
import csv
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Callable, Tuple
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass, asdict
import sqlite3
import pickle
import hashlib
import re
from collections import defaultdict, Counter
import statistics
from enum import Enum

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False

try:
    import openpyxl
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

class DataFormat(Enum):
    """æ•°æ®æ ¼å¼æšä¸¾"""
    CSV = "csv"
    JSON = "json"
    EXCEL = "excel"
    PARQUET = "parquet"
    PICKLE = "pickle"
    SQL = "sql"
    XML = "xml"
    TSV = "tsv"

class ProcessingStatus(Enum):
    """å¤„ç†çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class DataQualityReport:
    """æ•°æ®è´¨é‡æŠ¥å‘Š"""
    total_rows: int
    total_columns: int
    missing_values: Dict[str, int]
    duplicate_rows: int
    data_types: Dict[str, str]
    memory_usage: float
    quality_score: float
    issues: List[str]
    recommendations: List[str]
    timestamp: datetime

@dataclass
class ProcessingJob:
    """æ•°æ®å¤„ç†ä»»åŠ¡"""
    job_id: str
    name: str
    input_file: str
    output_file: str
    operations: List[Dict[str, Any]]
    status: ProcessingStatus
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    progress: float = 0.0
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, project_root: str = None):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨"""
        if project_root is None:
            self.project_root = Path(__file__).parent.parent.parent
        else:
            self.project_root = Path(project_root)
        
        # æ•°æ®ç›®å½•
        self.data_dir = self.project_root / "data"
        self.input_dir = self.data_dir / "input"
        self.output_dir = self.data_dir / "output"
        self.temp_dir = self.data_dir / "temp"
        self.cache_dir = self.data_dir / "cache"
        
        # åˆ›å»ºç›®å½•
        for directory in [self.data_dir, self.input_dir, self.output_dir, self.temp_dir, self.cache_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®æ–‡ä»¶
        self.config_file = self.data_dir / "processing_config.json"
        self.jobs_file = self.data_dir / "processing_jobs.json"
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.jobs = {}
        self.cache = {}
        self.processing_functions = {}
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger('yds_lab.data_processor')
        
        # åŠ è½½é…ç½®å’Œä»»åŠ¡
        self._load_config()
        self._load_jobs()
        
        # æ³¨å†Œé»˜è®¤å¤„ç†å‡½æ•°
        self._register_default_functions()
    
    def _load_config(self):
        """åŠ è½½é…ç½®"""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
            except Exception as e:
                self.logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
                self.config = {}
        else:
            self.config = {
                'max_memory_usage': 1024 * 1024 * 1024,  # 1GB
                'chunk_size': 10000,
                'cache_enabled': True,
                'auto_backup': True,
                'quality_threshold': 0.8,
                'supported_formats': ['csv', 'json', 'excel', 'parquet'],
                'default_encoding': 'utf-8'
            }
            self._save_config()
    
    def _save_config(self):
        """ä¿å­˜é…ç½®"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")
    
    def _load_jobs(self):
        """åŠ è½½å¤„ç†ä»»åŠ¡"""
        if self.jobs_file.exists():
            try:
                with open(self.jobs_file, 'r', encoding='utf-8') as f:
                    jobs_data = json.load(f)
                
                for job_id, job_data in jobs_data.items():
                    # è½¬æ¢æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²
                    for field in ['created_at', 'started_at', 'completed_at']:
                        if job_data.get(field):
                            job_data[field] = datetime.fromisoformat(job_data[field])
                    
                    # è½¬æ¢çŠ¶æ€æšä¸¾
                    job_data['status'] = ProcessingStatus(job_data['status'])
                    
                    self.jobs[job_id] = ProcessingJob(**job_data)
            
            except Exception as e:
                self.logger.error(f"åŠ è½½ä»»åŠ¡å¤±è´¥: {e}")
                self.jobs = {}
    
    def _save_jobs(self):
        """ä¿å­˜å¤„ç†ä»»åŠ¡"""
        try:
            jobs_data = {}
            for job_id, job in self.jobs.items():
                job_dict = asdict(job)
                
                # è½¬æ¢æ—¥æœŸæ—¶é—´ä¸ºå­—ç¬¦ä¸²
                for field in ['created_at', 'started_at', 'completed_at']:
                    if job_dict.get(field):
                        job_dict[field] = job_dict[field].isoformat()
                
                # è½¬æ¢çŠ¶æ€æšä¸¾ä¸ºå­—ç¬¦ä¸²
                job_dict['status'] = job_dict['status'].value
                
                jobs_data[job_id] = job_dict
            
            with open(self.jobs_file, 'w', encoding='utf-8') as f:
                json.dump(jobs_data, f, indent=2, ensure_ascii=False, default=str)
        
        except Exception as e:
            self.logger.error(f"ä¿å­˜ä»»åŠ¡å¤±è´¥: {e}")
    
    def _register_default_functions(self):
        """æ³¨å†Œé»˜è®¤å¤„ç†å‡½æ•°"""
        # æ•°æ®æ¸…æ´—å‡½æ•°
        self.processing_functions.update({
            'remove_duplicates': self._remove_duplicates,
            'fill_missing': self._fill_missing_values,
            'remove_outliers': self._remove_outliers,
            'normalize_text': self._normalize_text,
            'convert_types': self._convert_data_types,
            'filter_rows': self._filter_rows,
            'select_columns': self._select_columns,
            'rename_columns': self._rename_columns,
            'sort_data': self._sort_data,
            'group_by': self._group_by_operation,
            'merge_data': self._merge_data,
            'pivot_table': self._create_pivot_table,
            'calculate_stats': self._calculate_statistics,
            'create_features': self._create_features,
            'validate_data': self._validate_data
        })
    
    def load_data(self, file_path: str, format_type: DataFormat = None, **kwargs) -> pd.DataFrame:
        """åŠ è½½æ•°æ®"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
        if format_type is None:
            format_type = self._detect_format(file_path)
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_cache_key(file_path)
            if self.config.get('cache_enabled') and cache_key in self.cache:
                self.logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {file_path}")
                return self.cache[cache_key]
            
            self.logger.info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {file_path} (æ ¼å¼: {format_type.value})")
            
            # æ ¹æ®æ ¼å¼åŠ è½½æ•°æ®
            if format_type == DataFormat.CSV:
                df = pd.read_csv(file_path, encoding=kwargs.get('encoding', 'utf-8'), **kwargs)
            elif format_type == DataFormat.JSON:
                df = pd.read_json(file_path, encoding=kwargs.get('encoding', 'utf-8'), **kwargs)
            elif format_type == DataFormat.EXCEL:
                if not EXCEL_AVAILABLE:
                    raise ImportError("éœ€è¦å®‰è£… openpyxl æ¥å¤„ç†Excelæ–‡ä»¶")
                df = pd.read_excel(file_path, **kwargs)
            elif format_type == DataFormat.PARQUET:
                df = pd.read_parquet(file_path, **kwargs)
            elif format_type == DataFormat.PICKLE:
                df = pd.read_pickle(file_path)
            elif format_type == DataFormat.TSV:
                df = pd.read_csv(file_path, sep='\t', encoding=kwargs.get('encoding', 'utf-8'), **kwargs)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {format_type}")
            
            # ç¼“å­˜æ•°æ®
            if self.config.get('cache_enabled'):
                self.cache[cache_key] = df.copy()
            
            self.logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
            return df
        
        except Exception as e:
            self.logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            raise
    
    def save_data(self, df: pd.DataFrame, file_path: str, format_type: DataFormat = None, **kwargs) -> bool:
        """ä¿å­˜æ•°æ®"""
        file_path = Path(file_path)
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
        if format_type is None:
            format_type = self._detect_format(file_path)
        
        try:
            self.logger.info(f"ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶: {file_path} (æ ¼å¼: {format_type.value})")
            
            # å¤‡ä»½ç°æœ‰æ–‡ä»¶
            if file_path.exists() and self.config.get('auto_backup'):
                backup_path = file_path.with_suffix(f'.backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}{file_path.suffix}')
                file_path.rename(backup_path)
                self.logger.info(f"å·²å¤‡ä»½åŸæ–‡ä»¶: {backup_path}")
            
            # æ ¹æ®æ ¼å¼ä¿å­˜æ•°æ®
            if format_type == DataFormat.CSV:
                df.to_csv(file_path, index=False, encoding=kwargs.get('encoding', 'utf-8'), **kwargs)
            elif format_type == DataFormat.JSON:
                df.to_json(file_path, orient='records', force_ascii=False, indent=2, **kwargs)
            elif format_type == DataFormat.EXCEL:
                if not EXCEL_AVAILABLE:
                    raise ImportError("éœ€è¦å®‰è£… openpyxl æ¥å¤„ç†Excelæ–‡ä»¶")
                df.to_excel(file_path, index=False, **kwargs)
            elif format_type == DataFormat.PARQUET:
                df.to_parquet(file_path, index=False, **kwargs)
            elif format_type == DataFormat.PICKLE:
                df.to_pickle(file_path)
            elif format_type == DataFormat.TSV:
                df.to_csv(file_path, sep='\t', index=False, encoding=kwargs.get('encoding', 'utf-8'), **kwargs)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {format_type}")
            
            self.logger.info(f"æ•°æ®ä¿å­˜å®Œæˆ: {file_path}")
            return True
        
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def _detect_format(self, file_path: Path) -> DataFormat:
        """æ£€æµ‹æ–‡ä»¶æ ¼å¼"""
        suffix = file_path.suffix.lower()
        
        format_map = {
            '.csv': DataFormat.CSV,
            '.json': DataFormat.JSON,
            '.xlsx': DataFormat.EXCEL,
            '.xls': DataFormat.EXCEL,
            '.parquet': DataFormat.PARQUET,
            '.pkl': DataFormat.PICKLE,
            '.pickle': DataFormat.PICKLE,
            '.tsv': DataFormat.TSV,
            '.txt': DataFormat.TSV
        }
        
        return format_map.get(suffix, DataFormat.CSV)
    
    def _get_cache_key(self, file_path: Path) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        stat = file_path.stat()
        content = f"{file_path}_{stat.st_mtime}_{stat.st_size}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def analyze_data_quality(self, df: pd.DataFrame) -> DataQualityReport:
        """åˆ†ææ•°æ®è´¨é‡"""
        try:
            # åŸºæœ¬ç»Ÿè®¡
            total_rows = len(df)
            total_columns = len(df.columns)
            
            # ç¼ºå¤±å€¼ç»Ÿè®¡
            missing_values = df.isnull().sum().to_dict()
            
            # é‡å¤è¡Œç»Ÿè®¡
            duplicate_rows = df.duplicated().sum()
            
            # æ•°æ®ç±»å‹
            data_types = df.dtypes.astype(str).to_dict()
            
            # å†…å­˜ä½¿ç”¨
            memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
            
            # è´¨é‡é—®é¢˜æ£€æµ‹
            issues = []
            recommendations = []
            
            # æ£€æŸ¥ç¼ºå¤±å€¼
            missing_ratio = sum(missing_values.values()) / (total_rows * total_columns)
            if missing_ratio > 0.1:
                issues.append(f"ç¼ºå¤±å€¼æ¯”ä¾‹è¿‡é«˜: {missing_ratio:.2%}")
                recommendations.append("è€ƒè™‘å¡«å……æˆ–åˆ é™¤ç¼ºå¤±å€¼")
            
            # æ£€æŸ¥é‡å¤è¡Œ
            if duplicate_rows > 0:
                issues.append(f"å­˜åœ¨ {duplicate_rows} è¡Œé‡å¤æ•°æ®")
                recommendations.append("åˆ é™¤é‡å¤è¡Œ")
            
            # æ£€æŸ¥æ•°æ®ç±»å‹
            for col, dtype in data_types.items():
                if dtype == 'object':
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ˜¯æ•°å€¼ç±»å‹
                    try:
                        pd.to_numeric(df[col], errors='raise')
                        issues.append(f"åˆ— '{col}' å¯èƒ½åº”è¯¥æ˜¯æ•°å€¼ç±»å‹")
                        recommendations.append(f"è½¬æ¢åˆ— '{col}' ä¸ºæ•°å€¼ç±»å‹")
                    except:
                        pass
            
            # æ£€æŸ¥å¼‚å¸¸å€¼
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]
                
                if len(outliers) > total_rows * 0.05:  # è¶…è¿‡5%çš„å¼‚å¸¸å€¼
                    issues.append(f"åˆ— '{col}' å­˜åœ¨è¾ƒå¤šå¼‚å¸¸å€¼: {len(outliers)} ä¸ª")
                    recommendations.append(f"æ£€æŸ¥å¹¶å¤„ç†åˆ— '{col}' çš„å¼‚å¸¸å€¼")
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = 1.0
            quality_score -= missing_ratio * 0.3  # ç¼ºå¤±å€¼å½±å“
            quality_score -= (duplicate_rows / total_rows) * 0.2  # é‡å¤è¡Œå½±å“
            quality_score -= len(issues) * 0.1  # é—®é¢˜æ•°é‡å½±å“
            quality_score = max(0.0, min(1.0, quality_score))
            
            return DataQualityReport(
                total_rows=total_rows,
                total_columns=total_columns,
                missing_values=missing_values,
                duplicate_rows=duplicate_rows,
                data_types=data_types,
                memory_usage=memory_usage,
                quality_score=quality_score,
                issues=issues,
                recommendations=recommendations,
                timestamp=datetime.now()
            )
        
        except Exception as e:
            self.logger.error(f"æ•°æ®è´¨é‡åˆ†æå¤±è´¥: {e}")
            raise
    
    def create_processing_job(self, name: str, input_file: str, output_file: str, 
                            operations: List[Dict[str, Any]]) -> str:
        """åˆ›å»ºæ•°æ®å¤„ç†ä»»åŠ¡"""
        job_id = f"job_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(self.jobs)}"
        
        job = ProcessingJob(
            job_id=job_id,
            name=name,
            input_file=input_file,
            output_file=output_file,
            operations=operations,
            status=ProcessingStatus.PENDING,
            created_at=datetime.now()
        )
        
        self.jobs[job_id] = job
        self._save_jobs()
        
        self.logger.info(f"åˆ›å»ºå¤„ç†ä»»åŠ¡: {job_id} - {name}")
        return job_id
    
    def execute_job(self, job_id: str) -> bool:
        """æ‰§è¡Œå¤„ç†ä»»åŠ¡"""
        if job_id not in self.jobs:
            self.logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨: {job_id}")
            return False
        
        job = self.jobs[job_id]
        
        try:
            job.status = ProcessingStatus.PROCESSING
            job.started_at = datetime.now()
            job.progress = 0.0
            self._save_jobs()
            
            self.logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {job_id} - {job.name}")
            
            # åŠ è½½è¾“å…¥æ•°æ®
            df = self.load_data(job.input_file)
            job.progress = 10.0
            self._save_jobs()
            
            # æ‰§è¡Œæ“ä½œ
            total_operations = len(job.operations)
            for i, operation in enumerate(job.operations):
                operation_name = operation.get('name')
                operation_params = operation.get('params', {})
                
                self.logger.info(f"æ‰§è¡Œæ“ä½œ: {operation_name}")
                
                if operation_name in self.processing_functions:
                    df = self.processing_functions[operation_name](df, **operation_params)
                else:
                    raise ValueError(f"æœªçŸ¥çš„æ“ä½œ: {operation_name}")
                
                # æ›´æ–°è¿›åº¦
                job.progress = 10.0 + (i + 1) / total_operations * 80.0
                self._save_jobs()
            
            # ä¿å­˜è¾“å‡ºæ•°æ®
            self.save_data(df, job.output_file)
            job.progress = 100.0
            
            job.status = ProcessingStatus.COMPLETED
            job.completed_at = datetime.now()
            self._save_jobs()
            
            self.logger.info(f"ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {job_id}")
            return True
        
        except Exception as e:
            job.status = ProcessingStatus.FAILED
            job.error_message = str(e)
            job.completed_at = datetime.now()
            self._save_jobs()
            
            self.logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {job_id}: {e}")
            return False
    
    # æ•°æ®å¤„ç†å‡½æ•°
    def _remove_duplicates(self, df: pd.DataFrame, subset: List[str] = None, keep: str = 'first') -> pd.DataFrame:
        """åˆ é™¤é‡å¤è¡Œ"""
        return df.drop_duplicates(subset=subset, keep=keep)
    
    def _fill_missing_values(self, df: pd.DataFrame, method: str = 'mean', columns: List[str] = None) -> pd.DataFrame:
        """å¡«å……ç¼ºå¤±å€¼"""
        if columns is None:
            columns = df.columns
        
        df_copy = df.copy()
        
        for col in columns:
            if col not in df_copy.columns:
                continue
            
            if method == 'mean' and df_copy[col].dtype in ['int64', 'float64']:
                df_copy[col].fillna(df_copy[col].mean(), inplace=True)
            elif method == 'median' and df_copy[col].dtype in ['int64', 'float64']:
                df_copy[col].fillna(df_copy[col].median(), inplace=True)
            elif method == 'mode':
                df_copy[col].fillna(df_copy[col].mode().iloc[0] if not df_copy[col].mode().empty else '', inplace=True)
            elif method == 'forward':
                df_copy[col].fillna(method='ffill', inplace=True)
            elif method == 'backward':
                df_copy[col].fillna(method='bfill', inplace=True)
            elif isinstance(method, (str, int, float)):
                df_copy[col].fillna(method, inplace=True)
        
        return df_copy
    
    def _remove_outliers(self, df: pd.DataFrame, columns: List[str] = None, method: str = 'iqr') -> pd.DataFrame:
        """åˆ é™¤å¼‚å¸¸å€¼"""
        if columns is None:
            columns = df.select_dtypes(include=[np.number]).columns
        
        df_copy = df.copy()
        
        for col in columns:
            if col not in df_copy.columns:
                continue
            
            if method == 'iqr':
                Q1 = df_copy[col].quantile(0.25)
                Q3 = df_copy[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                df_copy = df_copy[(df_copy[col] >= lower_bound) & (df_copy[col] <= upper_bound)]
            elif method == 'zscore':
                z_scores = np.abs(statistics.zscore(df_copy[col].dropna()))
                df_copy = df_copy[z_scores < 3]
        
        return df_copy
    
    def _normalize_text(self, df: pd.DataFrame, columns: List[str] = None, operations: List[str] = None) -> pd.DataFrame:
        """æ–‡æœ¬æ ‡å‡†åŒ–"""
        if columns is None:
            columns = df.select_dtypes(include=['object']).columns
        
        if operations is None:
            operations = ['strip', 'lower']
        
        df_copy = df.copy()
        
        for col in columns:
            if col not in df_copy.columns:
                continue
            
            for operation in operations:
                if operation == 'strip':
                    df_copy[col] = df_copy[col].astype(str).str.strip()
                elif operation == 'lower':
                    df_copy[col] = df_copy[col].astype(str).str.lower()
                elif operation == 'upper':
                    df_copy[col] = df_copy[col].astype(str).str.upper()
                elif operation == 'title':
                    df_copy[col] = df_copy[col].astype(str).str.title()
                elif operation == 'remove_special':
                    df_copy[col] = df_copy[col].astype(str).str.replace(r'[^\w\s]', '', regex=True)
        
        return df_copy
    
    def _convert_data_types(self, df: pd.DataFrame, type_mapping: Dict[str, str]) -> pd.DataFrame:
        """è½¬æ¢æ•°æ®ç±»å‹"""
        df_copy = df.copy()
        
        for col, dtype in type_mapping.items():
            if col not in df_copy.columns:
                continue
            
            try:
                if dtype == 'int':
                    df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce').astype('Int64')
                elif dtype == 'float':
                    df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')
                elif dtype == 'datetime':
                    df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')
                elif dtype == 'category':
                    df_copy[col] = df_copy[col].astype('category')
                elif dtype == 'string':
                    df_copy[col] = df_copy[col].astype(str)
            except Exception as e:
                self.logger.warning(f"è½¬æ¢åˆ— {col} åˆ° {dtype} å¤±è´¥: {e}")
        
        return df_copy
    
    def _filter_rows(self, df: pd.DataFrame, conditions: List[Dict[str, Any]]) -> pd.DataFrame:
        """è¿‡æ»¤è¡Œ"""
        df_copy = df.copy()
        
        for condition in conditions:
            column = condition.get('column')
            operator = condition.get('operator')
            value = condition.get('value')
            
            if column not in df_copy.columns:
                continue
            
            if operator == '==':
                df_copy = df_copy[df_copy[column] == value]
            elif operator == '!=':
                df_copy = df_copy[df_copy[column] != value]
            elif operator == '>':
                df_copy = df_copy[df_copy[column] > value]
            elif operator == '>=':
                df_copy = df_copy[df_copy[column] >= value]
            elif operator == '<':
                df_copy = df_copy[df_copy[column] < value]
            elif operator == '<=':
                df_copy = df_copy[df_copy[column] <= value]
            elif operator == 'in':
                df_copy = df_copy[df_copy[column].isin(value)]
            elif operator == 'not_in':
                df_copy = df_copy[~df_copy[column].isin(value)]
            elif operator == 'contains':
                df_copy = df_copy[df_copy[column].astype(str).str.contains(str(value), na=False)]
        
        return df_copy
    
    def _select_columns(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
        """é€‰æ‹©åˆ—"""
        available_columns = [col for col in columns if col in df.columns]
        return df[available_columns]
    
    def _rename_columns(self, df: pd.DataFrame, column_mapping: Dict[str, str]) -> pd.DataFrame:
        """é‡å‘½ååˆ—"""
        return df.rename(columns=column_mapping)
    
    def _sort_data(self, df: pd.DataFrame, columns: List[str], ascending: Union[bool, List[bool]] = True) -> pd.DataFrame:
        """æ’åºæ•°æ®"""
        available_columns = [col for col in columns if col in df.columns]
        if available_columns:
            return df.sort_values(by=available_columns, ascending=ascending)
        return df
    
    def _group_by_operation(self, df: pd.DataFrame, group_columns: List[str], 
                          agg_operations: Dict[str, Union[str, List[str]]]) -> pd.DataFrame:
        """åˆ†ç»„æ“ä½œ"""
        available_group_columns = [col for col in group_columns if col in df.columns]
        if not available_group_columns:
            return df
        
        return df.groupby(available_group_columns).agg(agg_operations).reset_index()
    
    def _merge_data(self, df: pd.DataFrame, other_file: str, on: Union[str, List[str]], 
                   how: str = 'inner') -> pd.DataFrame:
        """åˆå¹¶æ•°æ®"""
        other_df = self.load_data(other_file)
        return df.merge(other_df, on=on, how=how)
    
    def _create_pivot_table(self, df: pd.DataFrame, index: Union[str, List[str]], 
                          columns: Union[str, List[str]], values: str, 
                          aggfunc: str = 'mean') -> pd.DataFrame:
        """åˆ›å»ºé€è§†è¡¨"""
        return df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)
    
    def _calculate_statistics(self, df: pd.DataFrame, columns: List[str] = None) -> pd.DataFrame:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if columns is None:
            columns = df.select_dtypes(include=[np.number]).columns
        
        stats_df = df[columns].describe()
        return stats_df
    
    def _create_features(self, df: pd.DataFrame, feature_definitions: List[Dict[str, Any]]) -> pd.DataFrame:
        """åˆ›å»ºç‰¹å¾"""
        df_copy = df.copy()
        
        for feature_def in feature_definitions:
            feature_name = feature_def.get('name')
            feature_type = feature_def.get('type')
            params = feature_def.get('params', {})
            
            if feature_type == 'arithmetic':
                # ç®—æœ¯è¿ç®—ç‰¹å¾
                expression = params.get('expression')
                try:
                    df_copy[feature_name] = df_copy.eval(expression)
                except Exception as e:
                    self.logger.warning(f"åˆ›å»ºç®—æœ¯ç‰¹å¾å¤±è´¥ {feature_name}: {e}")
            
            elif feature_type == 'datetime':
                # æ—¥æœŸæ—¶é—´ç‰¹å¾
                source_column = params.get('source_column')
                extract_part = params.get('extract_part')  # year, month, day, hour, etc.
                
                if source_column in df_copy.columns:
                    try:
                        dt_series = pd.to_datetime(df_copy[source_column])
                        df_copy[feature_name] = getattr(dt_series.dt, extract_part)
                    except Exception as e:
                        self.logger.warning(f"åˆ›å»ºæ—¥æœŸç‰¹å¾å¤±è´¥ {feature_name}: {e}")
            
            elif feature_type == 'categorical':
                # åˆ†ç±»ç‰¹å¾
                source_column = params.get('source_column')
                bins = params.get('bins')
                labels = params.get('labels')
                
                if source_column in df_copy.columns:
                    try:
                        df_copy[feature_name] = pd.cut(df_copy[source_column], bins=bins, labels=labels)
                    except Exception as e:
                        self.logger.warning(f"åˆ›å»ºåˆ†ç±»ç‰¹å¾å¤±è´¥ {feature_name}: {e}")
        
        return df_copy
    
    def _validate_data(self, df: pd.DataFrame, validation_rules: List[Dict[str, Any]]) -> pd.DataFrame:
        """æ•°æ®éªŒè¯"""
        df_copy = df.copy()
        validation_results = []
        
        for rule in validation_rules:
            rule_name = rule.get('name')
            rule_type = rule.get('type')
            params = rule.get('params', {})
            
            if rule_type == 'not_null':
                columns = params.get('columns', [])
                for col in columns:
                    if col in df_copy.columns:
                        null_count = df_copy[col].isnull().sum()
                        validation_results.append({
                            'rule': rule_name,
                            'column': col,
                            'passed': null_count == 0,
                            'details': f"ç©ºå€¼æ•°é‡: {null_count}"
                        })
            
            elif rule_type == 'range':
                column = params.get('column')
                min_value = params.get('min')
                max_value = params.get('max')
                
                if column in df_copy.columns:
                    out_of_range = df_copy[(df_copy[column] < min_value) | (df_copy[column] > max_value)]
                    validation_results.append({
                        'rule': rule_name,
                        'column': column,
                        'passed': len(out_of_range) == 0,
                        'details': f"è¶…å‡ºèŒƒå›´çš„è®°å½•æ•°: {len(out_of_range)}"
                    })
            
            elif rule_type == 'unique':
                column = params.get('column')
                
                if column in df_copy.columns:
                    duplicate_count = df_copy[column].duplicated().sum()
                    validation_results.append({
                        'rule': rule_name,
                        'column': column,
                        'passed': duplicate_count == 0,
                        'details': f"é‡å¤å€¼æ•°é‡: {duplicate_count}"
                    })
        
        # å°†éªŒè¯ç»“æœæ·»åŠ åˆ°å…ƒæ•°æ®
        df_copy.attrs['validation_results'] = validation_results
        
        return df_copy
    
    def generate_report(self, df: pd.DataFrame, output_file: str = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æŠ¥å‘Š"""
        try:
            # æ•°æ®è´¨é‡åˆ†æ
            quality_report = self.analyze_data_quality(df)
            
            # åŸºæœ¬ç»Ÿè®¡
            numeric_stats = df.describe().to_dict() if len(df.select_dtypes(include=[np.number]).columns) > 0 else {}
            
            # åˆ†ç±»ç»Ÿè®¡
            categorical_stats = {}
            for col in df.select_dtypes(include=['object', 'category']).columns:
                categorical_stats[col] = {
                    'unique_count': df[col].nunique(),
                    'top_values': df[col].value_counts().head(10).to_dict()
                }
            
            # ç›¸å…³æ€§åˆ†æ
            correlation_matrix = {}
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                correlation_matrix = df[numeric_cols].corr().to_dict()
            
            report = {
                'timestamp': datetime.now().isoformat(),
                'data_shape': {'rows': len(df), 'columns': len(df.columns)},
                'quality_report': asdict(quality_report),
                'numeric_statistics': numeric_stats,
                'categorical_statistics': categorical_stats,
                'correlation_matrix': correlation_matrix,
                'column_info': {
                    col: {
                        'dtype': str(df[col].dtype),
                        'null_count': int(df[col].isnull().sum()),
                        'null_percentage': float(df[col].isnull().sum() / len(df) * 100)
                    }
                    for col in df.columns
                }
            }
            
            # ä¿å­˜æŠ¥å‘Š
            if output_file:
                output_path = Path(output_file)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False, default=str)
                
                self.logger.info(f"æ•°æ®æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
            
            return report
        
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ•°æ®æŠ¥å‘Šå¤±è´¥: {e}")
            raise
    
    def create_visualization(self, df: pd.DataFrame, chart_type: str, output_file: str = None, **kwargs) -> bool:
        """åˆ›å»ºæ•°æ®å¯è§†åŒ–"""
        if not PLOTTING_AVAILABLE:
            self.logger.error("matplotlib å’Œ seaborn ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºå¯è§†åŒ–")
            return False
        
        try:
            plt.figure(figsize=kwargs.get('figsize', (10, 6)))
            
            if chart_type == 'histogram':
                column = kwargs.get('column')
                if column and column in df.columns:
                    plt.hist(df[column].dropna(), bins=kwargs.get('bins', 30))
                    plt.title(f'{column} åˆ†å¸ƒ')
                    plt.xlabel(column)
                    plt.ylabel('é¢‘æ¬¡')
            
            elif chart_type == 'scatter':
                x_col = kwargs.get('x_column')
                y_col = kwargs.get('y_column')
                if x_col and y_col and x_col in df.columns and y_col in df.columns:
                    plt.scatter(df[x_col], df[y_col], alpha=0.6)
                    plt.xlabel(x_col)
                    plt.ylabel(y_col)
                    plt.title(f'{x_col} vs {y_col}')
            
            elif chart_type == 'boxplot':
                column = kwargs.get('column')
                if column and column in df.columns:
                    plt.boxplot(df[column].dropna())
                    plt.title(f'{column} ç®±çº¿å›¾')
                    plt.ylabel(column)
            
            elif chart_type == 'correlation_heatmap':
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 1:
                    correlation_matrix = df[numeric_cols].corr()
                    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
                    plt.title('ç›¸å…³æ€§çƒ­åŠ›å›¾')
            
            elif chart_type == 'bar':
                column = kwargs.get('column')
                if column and column in df.columns:
                    value_counts = df[column].value_counts().head(kwargs.get('top_n', 10))
                    plt.bar(range(len(value_counts)), value_counts.values)
                    plt.xticks(range(len(value_counts)), value_counts.index, rotation=45)
                    plt.title(f'{column} åˆ†å¸ƒ')
                    plt.ylabel('è®¡æ•°')
            
            plt.tight_layout()
            
            if output_file:
                output_path = Path(output_file)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                self.logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {output_path}")
            
            plt.close()
            return True
        
        except Exception as e:
            self.logger.error(f"åˆ›å»ºå¯è§†åŒ–å¤±è´¥: {e}")
            plt.close()
            return False
    
    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        if job_id not in self.jobs:
            return None
        
        job = self.jobs[job_id]
        return {
            'job_id': job.job_id,
            'name': job.name,
            'status': job.status.value,
            'progress': job.progress,
            'created_at': job.created_at.isoformat(),
            'started_at': job.started_at.isoformat() if job.started_at else None,
            'completed_at': job.completed_at.isoformat() if job.completed_at else None,
            'error_message': job.error_message
        }
    
    def list_jobs(self, status_filter: ProcessingStatus = None) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
        jobs = []
        
        for job in self.jobs.values():
            if status_filter is None or job.status == status_filter:
                jobs.append({
                    'job_id': job.job_id,
                    'name': job.name,
                    'status': job.status.value,
                    'progress': job.progress,
                    'created_at': job.created_at.isoformat(),
                    'input_file': job.input_file,
                    'output_file': job.output_file
                })
        
        return sorted(jobs, key=lambda x: x['created_at'], reverse=True)
    
    def cancel_job(self, job_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        if job_id not in self.jobs:
            return False
        
        job = self.jobs[job_id]
        if job.status in [ProcessingStatus.PENDING, ProcessingStatus.PROCESSING]:
            job.status = ProcessingStatus.CANCELLED
            job.completed_at = datetime.now()
            self._save_jobs()
            
            self.logger.info(f"ä»»åŠ¡å·²å–æ¶ˆ: {job_id}")
            return True
        
        return False
    
    def delete_job(self, job_id: str) -> bool:
        """åˆ é™¤ä»»åŠ¡"""
        if job_id in self.jobs:
            del self.jobs[job_id]
            self._save_jobs()
            
            self.logger.info(f"ä»»åŠ¡å·²åˆ é™¤: {job_id}")
            return True
        
        return False
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.logger.info("æ•°æ®ç¼“å­˜å·²æ¸…ç©º")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='YDS-Lab æ•°æ®å¤„ç†å·¥å…·')
    parser.add_argument('--project-root', help='æŒ‡å®šé¡¹ç›®æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--load', help='åŠ è½½æ•°æ®æ–‡ä»¶')
    parser.add_argument('--format', choices=['csv', 'json', 'excel', 'parquet', 'pickle', 'tsv'], 
                       help='æŒ‡å®šæ•°æ®æ ¼å¼')
    parser.add_argument('--analyze', help='åˆ†ææ•°æ®è´¨é‡')
    parser.add_argument('--report', help='ç”Ÿæˆæ•°æ®æŠ¥å‘Š')
    parser.add_argument('--visualize', help='åˆ›å»ºå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--chart-type', choices=['histogram', 'scatter', 'boxplot', 'correlation_heatmap', 'bar'],
                       default='histogram', help='å›¾è¡¨ç±»å‹')
    parser.add_argument('--column', help='æŒ‡å®šåˆ—å')
    parser.add_argument('--x-column', help='Xè½´åˆ—å')
    parser.add_argument('--y-column', help='Yè½´åˆ—å')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--jobs', action='store_true', help='åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡')
    parser.add_argument('--job-status', help='æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€')
    parser.add_argument('--execute-job', help='æ‰§è¡Œä»»åŠ¡')
    parser.add_argument('--cancel-job', help='å–æ¶ˆä»»åŠ¡')
    parser.add_argument('--delete-job', help='åˆ é™¤ä»»åŠ¡')
    parser.add_argument('--clear-cache', action='store_true', help='æ¸…ç©ºç¼“å­˜')
    
    args = parser.parse_args()
    
    processor = DataProcessor(args.project_root)
    
    # åŠ è½½æ•°æ®
    if args.load:
        try:
            format_type = DataFormat(args.format) if args.format else None
            df = processor.load_data(args.load, format_type)
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
            
            # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            print("\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
            print(df.head())
            print(f"\nğŸ“‹ æ•°æ®ç±»å‹:")
            print(df.dtypes)
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ†ææ•°æ®è´¨é‡
    if args.analyze:
        try:
            format_type = DataFormat(args.format) if args.format else None
            df = processor.load_data(args.analyze, format_type)
            quality_report = processor.analyze_data_quality(df)
            
            print("ğŸ“Š æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š")
            print("="*50)
            print(f"æ€»è¡Œæ•°: {quality_report.total_rows}")
            print(f"æ€»åˆ—æ•°: {quality_report.total_columns}")
            print(f"é‡å¤è¡Œæ•°: {quality_report.duplicate_rows}")
            print(f"å†…å­˜ä½¿ç”¨: {quality_report.memory_usage:.2f} MB")
            print(f"è´¨é‡åˆ†æ•°: {quality_report.quality_score:.2f}")
            
            print(f"\nğŸ” ç¼ºå¤±å€¼ç»Ÿè®¡:")
            for col, count in quality_report.missing_values.items():
                if count > 0:
                    print(f"  {col}: {count}")
            
            if quality_report.issues:
                print(f"\nâš ï¸ å‘ç°çš„é—®é¢˜:")
                for issue in quality_report.issues:
                    print(f"  - {issue}")
            
            if quality_report.recommendations:
                print(f"\nğŸ’¡ å»ºè®®:")
                for rec in quality_report.recommendations:
                    print(f"  - {rec}")
            
        except Exception as e:
            print(f"âŒ æ•°æ®è´¨é‡åˆ†æå¤±è´¥: {e}")
        return
    
    # ç”Ÿæˆæ•°æ®æŠ¥å‘Š
    if args.report:
        try:
            format_type = DataFormat(args.format) if args.format else None
            df = processor.load_data(args.report, format_type)
            
            output_file = args.output or f"data_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            report = processor.generate_report(df, output_file)
            
            print(f"âœ… æ•°æ®æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            print(f"æ•°æ®å½¢çŠ¶: {report['data_shape']['rows']} è¡Œ x {report['data_shape']['columns']} åˆ—")
            print(f"è´¨é‡åˆ†æ•°: {report['quality_report']['quality_score']:.2f}")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ•°æ®æŠ¥å‘Šå¤±è´¥: {e}")
        return
    
    # åˆ›å»ºå¯è§†åŒ–
    if args.visualize:
        try:
            format_type = DataFormat(args.format) if args.format else None
            df = processor.load_data(args.visualize, format_type)
            
            output_file = args.output or f"visualization_{args.chart_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            
            kwargs = {}
            if args.column:
                kwargs['column'] = args.column
            if args.x_column:
                kwargs['x_column'] = args.x_column
            if args.y_column:
                kwargs['y_column'] = args.y_column
            
            success = processor.create_visualization(df, args.chart_type, output_file, **kwargs)
            
            if success:
                print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²åˆ›å»º: {output_file}")
            else:
                print("âŒ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨å¤±è´¥")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå¯è§†åŒ–å¤±è´¥: {e}")
        return
    
    # åˆ—å‡ºä»»åŠ¡
    if args.jobs:
        jobs = processor.list_jobs()
        
        if jobs:
            print("ğŸ“‹ å¤„ç†ä»»åŠ¡åˆ—è¡¨:")
            print("="*80)
            for job in jobs:
                print(f"ID: {job['job_id']}")
                print(f"åç§°: {job['name']}")
                print(f"çŠ¶æ€: {job['status']}")
                print(f"è¿›åº¦: {job['progress']:.1f}%")
                print(f"åˆ›å»ºæ—¶é—´: {job['created_at']}")
                print(f"è¾“å…¥æ–‡ä»¶: {job['input_file']}")
                print(f"è¾“å‡ºæ–‡ä»¶: {job['output_file']}")
                print("-" * 80)
        else:
            print("ğŸ“‹ æš‚æ— å¤„ç†ä»»åŠ¡")
        return
    
    # æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
    if args.job_status:
        status = processor.get_job_status(args.job_status)
        
        if status:
            print(f"ğŸ“Š ä»»åŠ¡çŠ¶æ€: {args.job_status}")
            print("="*40)
            print(f"åç§°: {status['name']}")
            print(f"çŠ¶æ€: {status['status']}")
            print(f"è¿›åº¦: {status['progress']:.1f}%")
            print(f"åˆ›å»ºæ—¶é—´: {status['created_at']}")
            if status['started_at']:
                print(f"å¼€å§‹æ—¶é—´: {status['started_at']}")
            if status['completed_at']:
                print(f"å®Œæˆæ—¶é—´: {status['completed_at']}")
            if status['error_message']:
                print(f"é”™è¯¯ä¿¡æ¯: {status['error_message']}")
        else:
            print(f"âŒ ä»»åŠ¡ä¸å­˜åœ¨: {args.job_status}")
        return
    
    # æ‰§è¡Œä»»åŠ¡
    if args.execute_job:
        success = processor.execute_job(args.execute_job)
        
        if success:
            print(f"âœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {args.execute_job}")
        else:
            print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {args.execute_job}")
        return
    
    # å–æ¶ˆä»»åŠ¡
    if args.cancel_job:
        success = processor.cancel_job(args.cancel_job)
        
        if success:
            print(f"âœ… ä»»åŠ¡å·²å–æ¶ˆ: {args.cancel_job}")
        else:
            print(f"âŒ å–æ¶ˆä»»åŠ¡å¤±è´¥: {args.cancel_job}")
        return
    
    # åˆ é™¤ä»»åŠ¡
    if args.delete_job:
        success = processor.delete_job(args.delete_job)
        
        if success:
            print(f"âœ… ä»»åŠ¡å·²åˆ é™¤: {args.delete_job}")
        else:
            print(f"âŒ åˆ é™¤ä»»åŠ¡å¤±è´¥: {args.delete_job}")
        return
    
    # æ¸…ç©ºç¼“å­˜
    if args.clear_cache:
        processor.clear_cache()
        print("âœ… æ•°æ®ç¼“å­˜å·²æ¸…ç©º")
        return
    
    # é»˜è®¤æ˜¾ç¤ºçŠ¶æ€
    print("ğŸ“Š æ•°æ®å¤„ç†å™¨")
    print("="*30)
    print(f"é¡¹ç›®è·¯å¾„: {processor.project_root}")
    print(f"æ•°æ®ç›®å½•: {processor.data_dir}")
    print(f"ç¼“å­˜é¡¹ç›®: {len(processor.cache)}")
    print(f"å¤„ç†ä»»åŠ¡: {len(processor.jobs)}")
    
    # æ˜¾ç¤ºæœ€è¿‘çš„ä»»åŠ¡
    recent_jobs = processor.list_jobs()[:5]
    if recent_jobs:
        print(f"\nğŸ“‹ æœ€è¿‘ä»»åŠ¡:")
        for job in recent_jobs:
            print(f"  {job['job_id']}: {job['name']} ({job['status']})")

if __name__ == "__main__":
    main()