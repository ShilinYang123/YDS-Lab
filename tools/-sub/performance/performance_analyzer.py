#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YDS-Lab æ€§èƒ½åˆ†æå·¥å…·
æä¾›ä»£ç æ€§èƒ½åˆ†æã€æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–å»ºè®®
é€‚é…YDS-Labé¡¹ç›®ç»“æ„å’ŒAI Agentåä½œéœ€æ±‚
"""

import os
import sys
import json
import yaml
import time
import cProfile
import pstats
import tracemalloc
import threading
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import logging
import ast
import re

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    function_name: str
    filename: str
    line_number: int
    call_count: int
    total_time: float
    cumulative_time: float
    per_call_time: float
    memory_usage: int = 0
    memory_peak: int = 0

@dataclass
class MemorySnapshot:
    """å†…å­˜å¿«ç…§æ•°æ®ç±»"""
    timestamp: str
    current_memory: int
    peak_memory: int
    memory_blocks: int
    top_allocations: List[Dict[str, Any]]

@dataclass
class PerformanceIssue:
    """æ€§èƒ½é—®é¢˜æ•°æ®ç±»"""
    severity: str  # low, medium, high, critical
    category: str  # time, memory, io, cpu
    function_name: str
    filename: str
    line_number: int
    description: str
    suggestion: str
    metrics: Dict[str, Any]

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, project_root: str = None):
        """åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨"""
        if project_root is None:
            self.project_root = Path(__file__).parent.parent.parent
        else:
            self.project_root = Path(project_root)
        
        self.config_dir = self.project_root / "Struc" / "GeneralOffice" / "config"
        self.logs_dir = self.project_root / "logs" / "performance"
        self.reports_dir = self.project_root / "reports" / "performance"
        self.profiles_dir = self.project_root / "profiles"
        
        # åˆ›å»ºç›®å½•
        for directory in [self.logs_dir, self.reports_dir, self.profiles_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # åˆ†æé…ç½®
        self.config = self._load_config()
        
        # æ€§èƒ½æ•°æ®
        self.metrics_history = deque(maxlen=self.config['history_size'])
        self.memory_snapshots = deque(maxlen=self.config['memory_history_size'])
        self.performance_issues = []
        
        # åˆ†æçŠ¶æ€
        self.profiling = False
        self.memory_tracing = False
        self.profiler = None
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # æ€§èƒ½é˜ˆå€¼
        self.thresholds = self.config['performance_thresholds']
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½æ€§èƒ½åˆ†æé…ç½®"""
        config_file = self.config_dir / "performance_config.yaml"
        
        default_config = {
            'history_size': 1000,
            'memory_history_size': 100,
            'profile_output_dir': 'profiles',
            'enable_memory_tracing': True,
            'enable_line_profiling': False,
            'performance_thresholds': {
                'slow_function_time': 1.0,  # ç§’
                'high_memory_usage': 100 * 1024 * 1024,  # 100MB
                'excessive_calls': 1000,
                'memory_leak_threshold': 10 * 1024 * 1024,  # 10MBå¢é•¿
                'cpu_intensive_threshold': 5.0  # ç§’
            },
            'analysis_patterns': {
                'inefficient_loops': [
                    r'for\s+\w+\s+in\s+range\(len\(',
                    r'while\s+.*len\(',
                ],
                'memory_leaks': [
                    r'global\s+\w+\s*=\s*\[\]',
                    r'\.append\(',
                    r'\.extend\(',
                ],
                'io_bottlenecks': [
                    r'open\(',
                    r'\.read\(',
                    r'\.write\(',
                    r'requests\.',
                ],
                'database_issues': [
                    r'SELECT\s+\*',
                    r'\.execute\(',
                    r'cursor\.',
                ]
            },
            'optimization_suggestions': {
                'slow_function': [
                    "è€ƒè™‘ä½¿ç”¨ç¼“å­˜æœºåˆ¶",
                    "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦",
                    "ä½¿ç”¨å¹¶è¡Œå¤„ç†",
                    "å‡å°‘ä¸å¿…è¦çš„è®¡ç®—"
                ],
                'high_memory': [
                    "ä½¿ç”¨ç”Ÿæˆå™¨ä»£æ›¿åˆ—è¡¨",
                    "åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¯¹è±¡",
                    "è€ƒè™‘ä½¿ç”¨å†…å­˜æ˜ å°„",
                    "ä¼˜åŒ–æ•°æ®ç»“æ„"
                ],
                'excessive_calls': [
                    "å‡å°‘å‡½æ•°è°ƒç”¨æ¬¡æ•°",
                    "åˆå¹¶ç›¸ä¼¼æ“ä½œ",
                    "ä½¿ç”¨æ‰¹å¤„ç†",
                    "ç¼“å­˜è®¡ç®—ç»“æœ"
                ]
            },
            'log_level': 'INFO',
            'auto_cleanup': True,
            'retention_days': 30
        }
        
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    default_config.update(user_config)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ€§èƒ½åˆ†æé…ç½®å¤±è´¥: {e}")
        
        return default_config
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.logs_dir / f"performance_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=getattr(logging, self.config['log_level']),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def start_profiling(self, sort_by: str = 'cumulative') -> str:
        """å¯åŠ¨æ€§èƒ½åˆ†æ"""
        if self.profiling:
            self.logger.warning("æ€§èƒ½åˆ†æå·²åœ¨è¿è¡Œä¸­")
            return None
        
        # å¯åŠ¨cProfile
        self.profiler = cProfile.Profile()
        self.profiler.enable()
        
        # å¯åŠ¨å†…å­˜è·Ÿè¸ª
        if self.config['enable_memory_tracing']:
            tracemalloc.start()
            self.memory_tracing = True
        
        self.profiling = True
        self.sort_by = sort_by
        
        self.logger.info("æ€§èƒ½åˆ†æå·²å¯åŠ¨")
        return "æ€§èƒ½åˆ†æå·²å¯åŠ¨"
    
    def stop_profiling(self) -> str:
        """åœæ­¢æ€§èƒ½åˆ†æå¹¶ç”ŸæˆæŠ¥å‘Š"""
        if not self.profiling:
            self.logger.warning("æ€§èƒ½åˆ†ææœªåœ¨è¿è¡Œ")
            return None
        
        # åœæ­¢cProfile
        self.profiler.disable()
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        profile_file = self.profiles_dir / f"profile_{timestamp}.prof"
        
        # ä¿å­˜profileæ–‡ä»¶
        self.profiler.dump_stats(str(profile_file))
        
        # åˆ†æprofileæ•°æ®
        stats = pstats.Stats(self.profiler)
        stats.sort_stats(self.sort_by)
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        self._extract_performance_metrics(stats)
        
        # åœæ­¢å†…å­˜è·Ÿè¸ª
        if self.memory_tracing:
            self._capture_memory_snapshot()
            tracemalloc.stop()
            self.memory_tracing = False
        
        self.profiling = False
        self.profiler = None
        
        self.logger.info(f"æ€§èƒ½åˆ†æå·²åœæ­¢ï¼ŒæŠ¥å‘Šä¿å­˜è‡³: {profile_file}")
        return str(profile_file)
    
    def _extract_performance_metrics(self, stats: pstats.Stats):
        """æå–æ€§èƒ½æŒ‡æ ‡"""
        timestamp = datetime.now().isoformat()
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        for func_info, (call_count, total_time, cumulative_time, callers) in stats.stats.items():
            filename, line_number, function_name = func_info
            
            # è¿‡æ»¤ç³»ç»Ÿå‡½æ•°
            if not self._should_analyze_function(filename, function_name):
                continue
            
            per_call_time = total_time / call_count if call_count > 0 else 0
            
            metrics = PerformanceMetrics(
                timestamp=timestamp,
                function_name=function_name,
                filename=filename,
                line_number=line_number,
                call_count=call_count,
                total_time=total_time,
                cumulative_time=cumulative_time,
                per_call_time=per_call_time
            )
            
            self.metrics_history.append(metrics)
            
            # æ£€æŸ¥æ€§èƒ½é—®é¢˜
            self._check_performance_issues(metrics)
    
    def _should_analyze_function(self, filename: str, function_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ†æè¯¥å‡½æ•°"""
        # è¿‡æ»¤ç³»ç»Ÿåº“å’Œç¬¬ä¸‰æ–¹åº“
        if any(path in filename for path in [
            'site-packages', 'lib/python', '<built-in>', '<string>',
            'importlib', 'pkgutil', 'threading', 'queue'
        ]):
            return False
        
        # è¿‡æ»¤ç‰¹æ®Šå‡½æ•°
        if function_name.startswith('_') and not function_name.startswith('__'):
            return False
        
        # åªåˆ†æé¡¹ç›®ç›¸å…³æ–‡ä»¶
        try:
            file_path = Path(filename)
            return str(self.project_root) in str(file_path.resolve())
        except Exception:
            return False
    
    def _capture_memory_snapshot(self):
        """æ•è·å†…å­˜å¿«ç…§"""
        if not tracemalloc.is_tracing():
            return
        
        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')
        
        current_memory = sum(stat.size for stat in top_stats)
        peak_memory = tracemalloc.get_traced_memory()[1]
        
        # è·å–å‰10ä¸ªå†…å­˜åˆ†é…
        top_allocations = []
        for stat in top_stats[:10]:
            top_allocations.append({
                'filename': stat.traceback.format()[0] if stat.traceback.format() else 'unknown',
                'size': stat.size,
                'count': stat.count
            })
        
        memory_snapshot = MemorySnapshot(
            timestamp=datetime.now().isoformat(),
            current_memory=current_memory,
            peak_memory=peak_memory,
            memory_blocks=len(top_stats),
            top_allocations=top_allocations
        )
        
        self.memory_snapshots.append(memory_snapshot)
    
    def _check_performance_issues(self, metrics: PerformanceMetrics):
        """æ£€æŸ¥æ€§èƒ½é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥æ…¢å‡½æ•°
        if metrics.total_time > self.thresholds['slow_function_time']:
            issue = PerformanceIssue(
                severity='high' if metrics.total_time > 5.0 else 'medium',
                category='time',
                function_name=metrics.function_name,
                filename=metrics.filename,
                line_number=metrics.line_number,
                description=f"å‡½æ•°æ‰§è¡Œæ—¶é—´è¿‡é•¿: {metrics.total_time:.2f}ç§’",
                suggestion=self._get_optimization_suggestion('slow_function'),
                metrics={'total_time': metrics.total_time, 'call_count': metrics.call_count}
            )
            issues.append(issue)
        
        # æ£€æŸ¥é¢‘ç¹è°ƒç”¨
        if metrics.call_count > self.thresholds['excessive_calls']:
            issue = PerformanceIssue(
                severity='medium',
                category='cpu',
                function_name=metrics.function_name,
                filename=metrics.filename,
                line_number=metrics.line_number,
                description=f"å‡½æ•°è°ƒç”¨æ¬¡æ•°è¿‡å¤š: {metrics.call_count}æ¬¡",
                suggestion=self._get_optimization_suggestion('excessive_calls'),
                metrics={'call_count': metrics.call_count, 'per_call_time': metrics.per_call_time}
            )
            issues.append(issue)
        
        # æ£€æŸ¥é«˜å†…å­˜ä½¿ç”¨
        if metrics.memory_usage > self.thresholds['high_memory_usage']:
            issue = PerformanceIssue(
                severity='high',
                category='memory',
                function_name=metrics.function_name,
                filename=metrics.filename,
                line_number=metrics.line_number,
                description=f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {metrics.memory_usage / (1024*1024):.1f}MB",
                suggestion=self._get_optimization_suggestion('high_memory'),
                metrics={'memory_usage': metrics.memory_usage}
            )
            issues.append(issue)
        
        self.performance_issues.extend(issues)
    
    def _get_optimization_suggestion(self, issue_type: str) -> str:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = self.config['optimization_suggestions'].get(issue_type, [])
        return "; ".join(suggestions) if suggestions else "éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
    
    def analyze_code_patterns(self, file_path: str) -> List[PerformanceIssue]:
        """åˆ†æä»£ç æ¨¡å¼"""
        issues = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ†æä½æ•ˆæ¨¡å¼
            for pattern_type, patterns in self.config['analysis_patterns'].items():
                for pattern in patterns:
                    matches = re.finditer(pattern, content, re.MULTILINE)
                    
                    for match in matches:
                        line_number = content[:match.start()].count('\n') + 1
                        
                        issue = PerformanceIssue(
                            severity='medium',
                            category=self._get_pattern_category(pattern_type),
                            function_name='unknown',
                            filename=file_path,
                            line_number=line_number,
                            description=f"å‘ç°{pattern_type}æ¨¡å¼: {match.group()}",
                            suggestion=self._get_pattern_suggestion(pattern_type),
                            metrics={'pattern': pattern, 'line': line_number}
                        )
                        
                        issues.append(issue)
        
        except Exception as e:
            self.logger.error(f"åˆ†æä»£ç æ¨¡å¼å¤±è´¥ {file_path}: {e}")
        
        return issues
    
    def _get_pattern_category(self, pattern_type: str) -> str:
        """è·å–æ¨¡å¼ç±»åˆ«"""
        category_map = {
            'inefficient_loops': 'cpu',
            'memory_leaks': 'memory',
            'io_bottlenecks': 'io',
            'database_issues': 'io'
        }
        return category_map.get(pattern_type, 'other')
    
    def _get_pattern_suggestion(self, pattern_type: str) -> str:
        """è·å–æ¨¡å¼ä¼˜åŒ–å»ºè®®"""
        suggestions = {
            'inefficient_loops': "ä½¿ç”¨enumerate()æˆ–ç›´æ¥è¿­ä»£ï¼Œé¿å…range(len())",
            'memory_leaks': "æ³¨æ„å…¨å±€å˜é‡å’Œå¾ªç¯å¼•ç”¨ï¼ŒåŠæ—¶æ¸…ç†ä¸éœ€è¦çš„å¯¹è±¡",
            'io_bottlenecks': "è€ƒè™‘å¼‚æ­¥IOã€æ‰¹å¤„ç†æˆ–ç¼“å­˜æœºåˆ¶",
            'database_issues': "ä¼˜åŒ–SQLæŸ¥è¯¢ï¼Œä½¿ç”¨ç´¢å¼•ï¼Œé¿å…SELECT *"
        }
        return suggestions.get(pattern_type, "éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    def analyze_project(self, include_patterns: List[str] = None) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        if include_patterns is None:
            include_patterns = ['*.py']
        
        analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'project_root': str(self.project_root),
            'files_analyzed': 0,
            'total_issues': 0,
            'issues_by_severity': defaultdict(int),
            'issues_by_category': defaultdict(int),
            'files_with_issues': [],
            'top_issues': []
        }
        
        # æ‰«æé¡¹ç›®æ–‡ä»¶
        all_issues = []
        
        for pattern in include_patterns:
            for file_path in self.project_root.rglob(pattern):
                if self._should_analyze_file(file_path):
                    file_issues = self.analyze_code_patterns(str(file_path))
                    
                    if file_issues:
                        analysis_results['files_with_issues'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'issues_count': len(file_issues),
                            'issues': [asdict(issue) for issue in file_issues]
                        })
                        
                        all_issues.extend(file_issues)
                    
                    analysis_results['files_analyzed'] += 1
        
        # ç»Ÿè®¡ç»“æœ
        analysis_results['total_issues'] = len(all_issues)
        
        for issue in all_issues:
            analysis_results['issues_by_severity'][issue.severity] += 1
            analysis_results['issues_by_category'][issue.category] += 1
        
        # è·å–æœ€ä¸¥é‡çš„é—®é¢˜
        severity_order = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}
        sorted_issues = sorted(all_issues, 
                             key=lambda x: severity_order.get(x.severity, 0), 
                             reverse=True)
        
        analysis_results['top_issues'] = [asdict(issue) for issue in sorted_issues[:10]]
        
        return analysis_results
    
    def _should_analyze_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ†æè¯¥æ–‡ä»¶"""
        # è·³è¿‡ç‰¹å®šç›®å½•
        skip_dirs = {'.git', '__pycache__', '.pytest_cache', 'node_modules', 
                    '.venv', 'venv', 'env', 'build', 'dist'}
        
        if any(part in skip_dirs for part in file_path.parts):
            return False
        
        # è·³è¿‡æµ‹è¯•æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        if 'test' in file_path.name.lower():
            return False
        
        return True
    
    def benchmark_function(self, func: Callable, *args, iterations: int = 100, **kwargs) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•å‡½æ•°"""
        results = {
            'function_name': func.__name__,
            'iterations': iterations,
            'times': [],
            'memory_usage': [],
            'total_time': 0,
            'avg_time': 0,
            'min_time': float('inf'),
            'max_time': 0,
            'std_dev': 0
        }
        
        # é¢„çƒ­
        for _ in range(min(10, iterations // 10)):
            func(*args, **kwargs)
        
        # åŸºå‡†æµ‹è¯•
        for i in range(iterations):
            # å†…å­˜è·Ÿè¸ª
            if self.config['enable_memory_tracing']:
                tracemalloc.start()
            
            start_time = time.perf_counter()
            result = func(*args, **kwargs)
            end_time = time.perf_counter()
            
            execution_time = end_time - start_time
            results['times'].append(execution_time)
            
            if self.config['enable_memory_tracing']:
                current, peak = tracemalloc.get_traced_memory()
                results['memory_usage'].append(peak)
                tracemalloc.stop()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        times = results['times']
        results['total_time'] = sum(times)
        results['avg_time'] = results['total_time'] / iterations
        results['min_time'] = min(times)
        results['max_time'] = max(times)
        
        # è®¡ç®—æ ‡å‡†å·®
        if len(times) > 1:
            mean = results['avg_time']
            variance = sum((t - mean) ** 2 for t in times) / (len(times) - 1)
            results['std_dev'] = variance ** 0.5
        
        # å†…å­˜ç»Ÿè®¡
        if results['memory_usage']:
            results['avg_memory'] = sum(results['memory_usage']) / len(results['memory_usage'])
            results['max_memory'] = max(results['memory_usage'])
        
        return results
    
    def compare_functions(self, functions: List[Tuple[str, Callable]], *args, iterations: int = 100, **kwargs) -> Dict[str, Any]:
        """æ¯”è¾ƒå¤šä¸ªå‡½æ•°çš„æ€§èƒ½"""
        comparison_results = {
            'timestamp': datetime.now().isoformat(),
            'iterations': iterations,
            'functions': {},
            'ranking': []
        }
        
        # æµ‹è¯•æ¯ä¸ªå‡½æ•°
        for name, func in functions:
            self.logger.info(f"åŸºå‡†æµ‹è¯•å‡½æ•°: {name}")
            
            try:
                results = self.benchmark_function(func, *args, iterations=iterations, **kwargs)
                results['name'] = name
                comparison_results['functions'][name] = results
            except Exception as e:
                self.logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥ {name}: {e}")
                comparison_results['functions'][name] = {
                    'name': name,
                    'error': str(e)
                }
        
        # æ’åºï¼ˆæŒ‰å¹³å‡æ—¶é—´ï¼‰
        valid_results = [(name, data) for name, data in comparison_results['functions'].items() 
                        if 'avg_time' in data]
        
        valid_results.sort(key=lambda x: x[1]['avg_time'])
        
        for i, (name, data) in enumerate(valid_results):
            comparison_results['ranking'].append({
                'rank': i + 1,
                'name': name,
                'avg_time': data['avg_time'],
                'relative_speed': data['avg_time'] / valid_results[0][1]['avg_time'] if valid_results else 1.0
            })
        
        return comparison_results
    
    def profile_memory_usage(self, duration: int = 60) -> Dict[str, Any]:
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not tracemalloc.is_tracing():
            tracemalloc.start()
        
        start_time = time.time()
        snapshots = []
        
        self.logger.info(f"å¼€å§‹ç›‘æ§å†…å­˜ä½¿ç”¨ï¼ŒæŒç»­{duration}ç§’")
        
        while time.time() - start_time < duration:
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')
            
            current_memory = sum(stat.size for stat in top_stats)
            peak_memory = tracemalloc.get_traced_memory()[1]
            
            snapshots.append({
                'timestamp': time.time(),
                'current_memory': current_memory,
                'peak_memory': peak_memory,
                'blocks_count': len(top_stats)
            })
            
            time.sleep(1)  # æ¯ç§’é‡‡æ ·ä¸€æ¬¡
        
        tracemalloc.stop()
        
        # åˆ†æå†…å­˜è¶‹åŠ¿
        if len(snapshots) > 1:
            memory_growth = snapshots[-1]['current_memory'] - snapshots[0]['current_memory']
            avg_memory = sum(s['current_memory'] for s in snapshots) / len(snapshots)
            max_memory = max(s['current_memory'] for s in snapshots)
            
            return {
                'duration': duration,
                'snapshots_count': len(snapshots),
                'memory_growth': memory_growth,
                'avg_memory': avg_memory,
                'max_memory': max_memory,
                'peak_memory': max(s['peak_memory'] for s in snapshots),
                'snapshots': snapshots,
                'potential_leak': memory_growth > self.thresholds['memory_leak_threshold']
            }
        
        return {'error': 'å†…å­˜ç›‘æ§æ•°æ®ä¸è¶³'}
    
    def generate_performance_report(self, output_format: str = 'markdown') -> str:
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        if output_format == 'markdown':
            return self._generate_markdown_report()
        elif output_format == 'html':
            return self._generate_html_report()
        elif output_format == 'json':
            return self._generate_json_report()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output_format}")
    
    def _generate_markdown_report(self) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        report = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report.append("# ğŸš€ æ€§èƒ½åˆ†ææŠ¥å‘Š")
        report.append("")
        report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**é¡¹ç›®è·¯å¾„**: {self.project_root}")
        report.append(f"**åˆ†æçŠ¶æ€**: {'è¿è¡Œä¸­' if self.profiling else 'å·²åœæ­¢'}")
        report.append("")
        
        # æ€§èƒ½æŒ‡æ ‡æ‘˜è¦
        if self.metrics_history:
            report.append("## ğŸ“Š æ€§èƒ½æŒ‡æ ‡æ‘˜è¦")
            report.append("")
            
            # æœ€æ…¢çš„å‡½æ•°
            slow_functions = sorted(self.metrics_history, 
                                  key=lambda x: x.total_time, reverse=True)[:10]
            
            if slow_functions:
                report.append("### ğŸŒ æœ€æ…¢å‡½æ•° (Top 10)")
                report.append("")
                report.append("| å‡½æ•°å | æ–‡ä»¶ | æ€»æ—¶é—´(s) | è°ƒç”¨æ¬¡æ•° | å¹³å‡æ—¶é—´(s) |")
                report.append("|--------|------|-----------|----------|-------------|")
                
                for metrics in slow_functions:
                    filename = Path(metrics.filename).name
                    report.append(f"| {metrics.function_name} | {filename} | {metrics.total_time:.3f} | {metrics.call_count} | {metrics.per_call_time:.6f} |")
                
                report.append("")
            
            # è°ƒç”¨æœ€é¢‘ç¹çš„å‡½æ•°
            frequent_functions = sorted(self.metrics_history, 
                                      key=lambda x: x.call_count, reverse=True)[:10]
            
            if frequent_functions:
                report.append("### ğŸ”„ è°ƒç”¨æœ€é¢‘ç¹å‡½æ•° (Top 10)")
                report.append("")
                report.append("| å‡½æ•°å | æ–‡ä»¶ | è°ƒç”¨æ¬¡æ•° | æ€»æ—¶é—´(s) | å¹³å‡æ—¶é—´(s) |")
                report.append("|--------|------|----------|-----------|-------------|")
                
                for metrics in frequent_functions:
                    filename = Path(metrics.filename).name
                    report.append(f"| {metrics.function_name} | {filename} | {metrics.call_count} | {metrics.total_time:.3f} | {metrics.per_call_time:.6f} |")
                
                report.append("")
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        if self.memory_snapshots:
            report.append("## ğŸ’¾ å†…å­˜ä½¿ç”¨æƒ…å†µ")
            report.append("")
            
            latest_snapshot = self.memory_snapshots[-1]
            
            report.append(f"- **å½“å‰å†…å­˜ä½¿ç”¨**: {latest_snapshot.current_memory / (1024*1024):.1f} MB")
            report.append(f"- **å³°å€¼å†…å­˜ä½¿ç”¨**: {latest_snapshot.peak_memory / (1024*1024):.1f} MB")
            report.append(f"- **å†…å­˜å—æ•°é‡**: {latest_snapshot.memory_blocks}")
            report.append("")
            
            if latest_snapshot.top_allocations:
                report.append("### ğŸ” ä¸»è¦å†…å­˜åˆ†é…")
                report.append("")
                
                for i, alloc in enumerate(latest_snapshot.top_allocations[:5], 1):
                    size_mb = alloc['size'] / (1024*1024)
                    report.append(f"{i}. **{alloc['filename']}**: {size_mb:.2f} MB ({alloc['count']} ä¸ªå¯¹è±¡)")
                
                report.append("")
        
        # æ€§èƒ½é—®é¢˜
        if self.performance_issues:
            report.append("## âš ï¸ æ€§èƒ½é—®é¢˜")
            report.append("")
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
            issues_by_severity = defaultdict(list)
            for issue in self.performance_issues:
                issues_by_severity[issue.severity].append(issue)
            
            severity_order = ['critical', 'high', 'medium', 'low']
            severity_icons = {
                'critical': 'ğŸ”¥',
                'high': 'âŒ',
                'medium': 'âš ï¸',
                'low': 'â„¹ï¸'
            }
            
            for severity in severity_order:
                if severity in issues_by_severity:
                    issues = issues_by_severity[severity]
                    icon = severity_icons.get(severity, 'â“')
                    
                    report.append(f"### {icon} {severity.upper()} çº§åˆ«é—®é¢˜ ({len(issues)}ä¸ª)")
                    report.append("")
                    
                    for issue in issues[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                        filename = Path(issue.filename).name if issue.filename else 'unknown'
                        
                        report.append(f"**{issue.function_name}** ({filename}:{issue.line_number})")
                        report.append(f"- ç±»åˆ«: {issue.category}")
                        report.append(f"- é—®é¢˜: {issue.description}")
                        report.append(f"- å»ºè®®: {issue.suggestion}")
                        report.append("")
        
        # ä¼˜åŒ–å»ºè®®
        report.append("## ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        report.append("")
        
        if self.performance_issues:
            # ç»Ÿè®¡é—®é¢˜ç±»åˆ«
            category_counts = defaultdict(int)
            for issue in self.performance_issues:
                category_counts[issue.category] += 1
            
            if category_counts:
                report.append("### é—®é¢˜åˆ†å¸ƒ")
                report.append("")
                
                for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
                    report.append(f"- **{category}**: {count} ä¸ªé—®é¢˜")
                
                report.append("")
            
            # é€šç”¨ä¼˜åŒ–å»ºè®®
            report.append("### é€šç”¨ä¼˜åŒ–ç­–ç•¥")
            report.append("")
            report.append("1. **ä»£ç ä¼˜åŒ–**")
            report.append("   - ä½¿ç”¨æ›´é«˜æ•ˆçš„ç®—æ³•å’Œæ•°æ®ç»“æ„")
            report.append("   - é¿å…ä¸å¿…è¦çš„å¾ªç¯å’Œè®¡ç®—")
            report.append("   - ä½¿ç”¨å†…ç½®å‡½æ•°å’Œåº“å‡½æ•°")
            report.append("")
            report.append("2. **å†…å­˜ä¼˜åŒ–**")
            report.append("   - ä½¿ç”¨ç”Ÿæˆå™¨ä»£æ›¿å¤§åˆ—è¡¨")
            report.append("   - åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¯¹è±¡")
            report.append("   - è€ƒè™‘ä½¿ç”¨__slots__å‡å°‘å†…å­˜å ç”¨")
            report.append("")
            report.append("3. **å¹¶å‘ä¼˜åŒ–**")
            report.append("   - ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†I/Oå¯†é›†å‹ä»»åŠ¡")
            report.append("   - ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†CPUå¯†é›†å‹ä»»åŠ¡")
            report.append("   - è€ƒè™‘å¼‚æ­¥ç¼–ç¨‹æ¨¡å¼")
            report.append("")
        else:
            report.append("æš‚æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜ï¼Œç»§ç»­ä¿æŒè‰¯å¥½çš„ç¼–ç ä¹ æƒ¯ï¼")
            report.append("")
        
        return '\n'.join(report)
    
    def _generate_json_report(self) -> str:
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'project_root': str(self.project_root),
            'profiling_status': self.profiling,
            'metrics_count': len(self.metrics_history),
            'memory_snapshots_count': len(self.memory_snapshots),
            'performance_issues_count': len(self.performance_issues),
            'metrics': [asdict(m) for m in self.metrics_history],
            'memory_snapshots': [asdict(s) for s in self.memory_snapshots],
            'performance_issues': [asdict(i) for i in self.performance_issues]
        }
        
        return json.dumps(report_data, ensure_ascii=False, indent=2)
    
    def _generate_html_report(self) -> str:
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        # ç®€åŒ–çš„HTMLæŠ¥å‘Š
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>æ€§èƒ½åˆ†ææŠ¥å‘Š</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f5f5f5; padding: 20px; border-radius: 5px; }}
        .metrics {{ display: flex; gap: 20px; margin: 20px 0; }}
        .metric-card {{ background: #fff; border: 1px solid #ddd; padding: 15px; border-radius: 5px; flex: 1; }}
        .issue {{ padding: 10px; margin: 10px 0; border-radius: 5px; }}
        .issue.critical {{ background: #f8d7da; border: 1px solid #f5c6cb; }}
        .issue.high {{ background: #fff3cd; border: 1px solid #ffeaa7; }}
        .issue.medium {{ background: #d1ecf1; border: 1px solid #bee5eb; }}
        .issue.low {{ background: #d4edda; border: 1px solid #c3e6cb; }}
        table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ æ€§èƒ½åˆ†ææŠ¥å‘Š</h1>
        <p><strong>ç”Ÿæˆæ—¶é—´</strong>: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p><strong>åˆ†æçŠ¶æ€</strong>: {'è¿è¡Œä¸­' if self.profiling else 'å·²åœæ­¢'}</p>
    </div>
"""
        
        # æ€§èƒ½æŒ‡æ ‡
        if self.metrics_history:
            html += """
    <h2>ğŸ“Š æ€§èƒ½æŒ‡æ ‡æ‘˜è¦</h2>
    <div class="metrics">
        <div class="metric-card">
            <h3>å‡½æ•°æ•°é‡</h3>
            <p>{}</p>
        </div>
        <div class="metric-card">
            <h3>æ€§èƒ½é—®é¢˜</h3>
            <p>{}</p>
        </div>
        <div class="metric-card">
            <h3>å†…å­˜å¿«ç…§</h3>
            <p>{}</p>
        </div>
    </div>
""".format(len(self.metrics_history), len(self.performance_issues), len(self.memory_snapshots))
        
        # æ€§èƒ½é—®é¢˜
        if self.performance_issues:
            html += "<h2>âš ï¸ æ€§èƒ½é—®é¢˜</h2>"
            
            for issue in self.performance_issues[:10]:
                html += f"""
    <div class="issue {issue.severity}">
        <strong>{issue.function_name}</strong> ({issue.category.upper()})
        <p>{issue.description}</p>
        <p><em>å»ºè®®: {issue.suggestion}</em></p>
    </div>
"""
        
        html += """
</body>
</html>
"""
        
        return html
    
    def save_report(self, output_format: str = 'markdown', filename: str = None) -> str:
        """ä¿å­˜æ€§èƒ½åˆ†ææŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"performance_report_{timestamp}.{output_format}"
        
        report_content = self.generate_performance_report(output_format)
        report_path = self.reports_dir / filename
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“„ æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return str(report_path)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='YDS-Lab æ€§èƒ½åˆ†æå·¥å…·')
    parser.add_argument('--project-root', help='æŒ‡å®šé¡¹ç›®æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--profile', action='store_true', help='å¯åŠ¨æ€§èƒ½åˆ†æ')
    parser.add_argument('--analyze', help='åˆ†ææŒ‡å®šæ–‡ä»¶æˆ–ç›®å½•')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š')
    parser.add_argument('--format', choices=['markdown', 'html', 'json'], default='markdown', help='æŠ¥å‘Šæ ¼å¼')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--benchmark', help='åŸºå‡†æµ‹è¯•æŒ‡å®šå‡½æ•°')
    parser.add_argument('--memory', type=int, help='ç›‘æ§å†…å­˜ä½¿ç”¨ï¼ˆç§’ï¼‰')
    parser.add_argument('--duration', type=int, default=60, help='åˆ†ææŒç»­æ—¶é—´')
    
    args = parser.parse_args()
    
    analyzer = PerformanceAnalyzer(args.project_root)
    
    # å¯åŠ¨æ€§èƒ½åˆ†æ
    if args.profile:
        print("ğŸš€ å¯åŠ¨æ€§èƒ½åˆ†æ...")
        analyzer.start_profiling()
        
        try:
            print(f"â±ï¸ åˆ†æè¿è¡Œä¸­ï¼ŒæŒç»­ {args.duration} ç§’...")
            time.sleep(args.duration)
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
        finally:
            profile_file = analyzer.stop_profiling()
            print(f"âœ… æ€§èƒ½åˆ†æå®Œæˆ: {profile_file}")
        
        return
    
    # åˆ†æä»£ç 
    if args.analyze:
        analyze_path = Path(args.analyze)
        
        if analyze_path.is_file():
            print(f"ğŸ” åˆ†ææ–‡ä»¶: {analyze_path}")
            issues = analyzer.analyze_code_patterns(str(analyze_path))
            
            if issues:
                print(f"å‘ç° {len(issues)} ä¸ªæ€§èƒ½é—®é¢˜:")
                for issue in issues:
                    print(f"  {issue.severity.upper()}: {issue.description}")
            else:
                print("âœ… æœªå‘ç°æ€§èƒ½é—®é¢˜")
        
        elif analyze_path.is_dir():
            print(f"ğŸ” åˆ†æç›®å½•: {analyze_path}")
            results = analyzer.analyze_project(['*.py'])
            
            print(f"åˆ†æå®Œæˆ:")
            print(f"  æ–‡ä»¶æ•°é‡: {results['files_analyzed']}")
            print(f"  é—®é¢˜æ€»æ•°: {results['total_issues']}")
            
            for severity, count in results['issues_by_severity'].items():
                print(f"  {severity.upper()}: {count}")
        
        else:
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {analyze_path}")
        
        return
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.report:
        report_path = analyzer.save_report(args.format, args.output)
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return
    
    # å†…å­˜ç›‘æ§
    if args.memory:
        print(f"ğŸ’¾ å¼€å§‹ç›‘æ§å†…å­˜ä½¿ç”¨ï¼ŒæŒç»­ {args.memory} ç§’...")
        results = analyzer.profile_memory_usage(args.memory)
        
        if 'error' not in results:
            print(f"å†…å­˜ç›‘æ§å®Œæˆ:")
            print(f"  å¹³å‡å†…å­˜: {results['avg_memory'] / (1024*1024):.1f} MB")
            print(f"  æœ€å¤§å†…å­˜: {results['max_memory'] / (1024*1024):.1f} MB")
            print(f"  å†…å­˜å¢é•¿: {results['memory_growth'] / (1024*1024):.1f} MB")
            
            if results['potential_leak']:
                print("âš ï¸ æ£€æµ‹åˆ°æ½œåœ¨å†…å­˜æ³„æ¼")
        else:
            print(f"âŒ å†…å­˜ç›‘æ§å¤±è´¥: {results['error']}")
        
        return
    
    # é»˜è®¤æ˜¾ç¤ºçŠ¶æ€
    print("ğŸ“Š æ€§èƒ½åˆ†æå™¨çŠ¶æ€")
    print("="*40)
    print(f"é¡¹ç›®è·¯å¾„: {analyzer.project_root}")
    print(f"åˆ†æçŠ¶æ€: {'è¿è¡Œä¸­' if analyzer.profiling else 'å·²åœæ­¢'}")
    print(f"æ€§èƒ½æŒ‡æ ‡: {len(analyzer.metrics_history)} æ¡")
    print(f"å†…å­˜å¿«ç…§: {len(analyzer.memory_snapshots)} ä¸ª")
    print(f"æ€§èƒ½é—®é¢˜: {len(analyzer.performance_issues)} ä¸ª")

if __name__ == "__main__":
    main()